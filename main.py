# DO NOT EDIT THIS FILE, EDIT CONFIG.PY INSTEAD
from itertools import chain
from math import floor, ceil
import copy
import shlex
import shutil
import subprocess
from time import sleep
import asyncio
import config
import logging.handlers
import os
import pyboinc
from pyboinc.pyboinc._parse import parse_generic
from pyboinc import init_rpc_client
import xml.etree.ElementTree as ET
import json
import pprint
import re
import platform
from pathlib import Path
import datetime
import xmltodict
import requests
from requests.auth import HTTPBasicAuth
from typing import List, Union, Dict, Tuple, Any
import sys, signal

# ignore deprecation warnings in Windows
import warnings

warnings.filterwarnings('ignore', category = DeprecationWarning)

# Set default settings for all vars
preferred_projects_percent: float = 80
preferred_projects: Dict[str, int] = {}
ignored_projects: List[str] = ['https://foldingathome.div72.xyz/']
boinc_data_dir: Union[str, None] = None
gridcoin_data_dir: Union[str, None] = None
control_boinc: bool = False
boinc_ip: str = '127.0.0.1'
boinc_port: int = 31416
boinc_username: Union[str, None] = None
boinc_password: Union[str, None] = None
min_recheck_time: int = 30  # minimum time in minutes before re-asking a project for work who previously said they were out
abort_unstarted_tasks: bool = False
recalculate_stats_interval: int = 60
price_check_interval: int = 720
local_kwh: float = 0.1542
grc_sell_price: Union[float, None] = None
exchange_fee: float = 0.00
only_BOINC_if_profitable: bool = False
only_mine_if_profitable: bool = False
host_power_usage: float = 70
min_profit_per_hour: float = 0
benchmarking_minimum_wus: float = 5
benchmarking_minimum_time: float = 10
benchmarking_delay_in_days: float = 160
skip_benchmarking: bool = False
VERSION = 2.2
log_level = 'WARNING'
start_temp: int = 65
stop_temp: int = 75
temp_command = None
enable_temp_control = True  # Enable controlling BOINC based on temp. Default: False
temp_sleep_time = 10
temp_regex = r'\d*'
max_logfile_size_in_mb = 10
rolling_weight_window = 60
lookback_period = 30

# Some globals we need. I try to have all globals be ALL CAPS
BOINC_PROJECT_NAMES = {}
DATABASE = {}
DATABASE['TABLE_SLEEP_REASON'] = ''  # sleep reason printed in table, must be reset at script start
DATABASE['TABLE_STATUS'] = ''  # info status printed in table, must be reset at script start
SCRIPTED_RUN: bool = False
SKIP_TABLE_UPDATES: bool = False
HOST_COST_PER_HOUR = (host_power_usage / 1000) * local_kwh
# Translates BOINC's CPU and GPU Mode replies into English. Note difference between keys integer vs string.
CPU_MODE_DICT = {1: 'always', 2: 'auto', 3: 'never'}
GPU_MODE_DICT = {'1': 'always', '2': 'auto', '3': 'never'}

# import user settings from config
try:
    from config import *
except Exception as e:
    print('Error opening config.py, using defaults! Error is: {}'.format(e))
# if user has no preferred projects, their % of crunching should be 0
if len(preferred_projects) == 0:
    preferred_projects_percent: float = 0

# setup logging
log = logging.getLogger()
if log_level == 'NONE':
    log.addHandler(logging.NullHandler())
else:
    handler = logging.handlers.RotatingFileHandler(
        os.environ.get("LOGFILE", "debug.log"), maxBytes = max_logfile_size_in_mb * 1024 * 1024, backupCount = 1
        )
    log.setLevel(os.environ.get("LOGLEVEL", log_level))
    formatter = logging.Formatter(logging.BASIC_FORMAT)
    handler.setFormatter(formatter)
    log.addHandler(handler)


class GridcoinClientConnection:
    """
    A class for connecting to a Gridcoin wallet and issuing RPC commands. Currently quite barebones.
    """

    def __init__(
        self,
        config_file: str = None,
        ip_address: str = '127.0.0.1',
        rpc_port: str = '9876',
        rpc_user: str = None,
        rpc_password: str = None,
        ):
        self.configfile = config_file  #absolute path to the client config file
        self.ipaddress = ip_address
        self.rpc_port = rpc_port
        self.rpcuser = rpc_user
        self.rpcpassword = rpc_password

    def run_command(self, command: str, arguments: List[Union[str, bool]] = None) -> dict:
        if not arguments:
            arguments = []
        credentials = None
        url = 'http://' + self.ipaddress + ':' + self.rpc_port + '/'
        headers = {'content-type': 'application/json'}
        payload = {
            "method": command,
            "params": arguments,
            "jsonrpc": "2.0",
            "id": 0,
            }
        jsonpayload = json.dumps(payload, default = json_default)
        if self.rpcuser or self.rpcpassword:
            credentials = HTTPBasicAuth(self.rpcuser, self.rpcpassword)
        response = requests.post(url, data = jsonpayload, headers = headers, auth = credentials)
        return response.json()

    def get_approved_project_urls(self) -> List[str]:
        """
        :return: A list of UPPERCASED project URLs using gridcoin command listprojects
        """
        return_list = []
        all_projects = self.run_command('listprojects')
        for projectname, project in all_projects['result'].items():
            return_list.append(project['base_url'].upper())
        return return_list

    def project_name_to_url(self, searchname: str) -> Union[str, None]:
        """
        Convert a project name into its project url, then UPPERCASE it
        """
        all_projects = self.run_command('listprojects')
        for found_project_name, project_dict in all_projects['result'].items():
            if found_project_name.upper() == searchname.upper():
                return project_dict['base_url'].upper()
        return None


class BoincClientConnection:
    """
    A simple class for grepping BOINC config files etc. Doesn't do any RPC communication
    """

    def __init__(
        self,
        config_dir: str = None,
        ip_address: str = '127.0.0.1',
        port: str = '9876',
        rpc_user: str = boinc_username,
        rpc_password: str = None
        ):
        if config_dir is None:
            self.config_dir = '/var/lib/boinc-client'
        else:
            self.config_dir = config_dir  # absolute path to the client config dir
        self.ip_address = ip_address
        self.port = port
        self.rpc_user = rpc_user
        self.rpc_password = rpc_password

    def get_project_list(self) -> List[str]:
        """
        :return: UPPERCASED list of project URLs. This is all of them, not just ones which are attached
        """
        project_list_file = os.path.join(self.config_dir, 'all_projects_list.xml')
        return_list = []
        with open(project_list_file, mode = 'r', encoding = 'ASCII', errors = 'ignore') as f:
            parsed = xmltodict.parse(f.read())
            for project in parsed['projects']['project']:
                return_list.append(project['url'].upper())
        return return_list


def safe_exit(arg1, arg2) -> None:
    """
    Function to safely exit tool by saving database, restoring original user preferences.
    arg1/2 required by the signal handler library, but aren't used for anything inside this function
    """

    new_loop = asyncio.get_event_loop()
    # this is needed in case this function is called while main loop is still waiting for an RPC command etc
    print_and_log("Program exiting gracefully", 'INFO')

    # Backup most recent database save then save database to json file
    log.debug('Saving database')
    shutil.copy('stats.json', 'stats.json.backup')
    save_stats(DATABASE)

    # If BOINC control is not enabled, we can skip the rest of these steps
    if not control_boinc:
        quit()

    # Restore original BOINC preferences
    if os.path.exists(override_dest_path):
        print('Restoring original preferences...')
        log.debug('Restoring original preferences...')
        try:
            shutil.copy(override_dest_path, override_path)
        except PermissionError as e:
            print('Permission error restoring original BOINC preferences {}'.format(e))
            log.error('Permission error restoring original BOINC preferences {}'.format(e))
            print('Be sure you have permission to edit this file')
            print("Linux users try  'sudo usermod -aG boinc your_username_here' to fix this error".format(override_path))
            print('Note that you will need to restart your machine for these changes to take effect')
        except Exception as e:
            print('Error restoring original BOINC preferences {}'.format(e))
            log.error('Error restoring original BOINC preferences {}'.format(e))
            print('Be sure you have permission to edit this file')
            print("Linux users try  'sudo usermod -aG boinc your_username_here' to fix this error".format(override_path))
            print('Note that you will need to restart your machine for these changes to take effect')
        else:
            os.remove(override_dest_path)

    quit()


async def get_task_list(rpc_client: pyboinc.rpc_client) -> list:
    """
    Return list of tasks from BOINC client which are not completed/failed. These
    can be active tasks, tasks waiting to be started, or paused tasks.
    """
    # Known task states
    # 2: Active
    return_value = []
    reply = await run_rpc_command(rpc_client, 'get_results')
    if isinstance(reply, str):
        log.info('BOINC appears to have no tasks...')
        return return_value
    for task in reply:
        if task['state'] in [2]:
            return_value.append(task)
        else:
            log.warning('Warning: Found unknown task state {}: {}'.format(task['state'], task))
    return return_value


async def is_boinc_crunching(rpc_client: pyboinc.rpc_client) -> bool:
    """
    Returns True is boinc is crunching, false otherwise
    """
    reply = await run_rpc_command(rpc_client, 'get_cc_status')
    task_suspend_reason = int(reply['task_suspend_reason'])
    if task_suspend_reason != 0:
        # These are documented at https://github.com/BOINC/boinc/blob/73a7754e7fd1ae3b7bf337e8dd42a7a0b42cf3d2/android/BOINC/app/src/main/java/edu/berkeley/boinc/utils/BOINCDefs.kt
        log.debug('Determined BOINC client is not crunching task_suspend_reason: {}'.format(task_suspend_reason))
        return False
    if task_suspend_reason == 0:
        log.debug('Determined BOINC client is crunching task_suspend_reason: {}'.format(task_suspend_reason))
        return True
    log.warning('Unable to determine if BOINC is crunching or not, assuming not.')
    return False


async def setup_connection(
    boinc_ip: str = boinc_ip, boinc_password: str = boinc_password, port: int = 31416
    ) -> pyboinc.rpc_client:
    """
    Sets up a BOINC RPC client connection
    """
    rpc_client = await init_rpc_client(boinc_ip, boinc_password, port = port)
    return rpc_client


def temp_check() -> bool:
    """
    Returns True if we should keep crunching based on temperature, False otherwise
    """
    if not enable_temp_control:
        return True
    text = ''
    if temp_url:
        import requests as req
        try:
            text = req.get(temp_url).text
        except Exception as e:
            print('Error checking temp: {}'.format(e))
            log.error('Error checking temp: {}'.format(e))
            return True
    elif temp_command:
        command = shlex.split(temp_command)
        try:
            text = subprocess.check_output(command)
        except Exception as e:
            print('Error checking temp: {}'.format(e))
            log.error('Error checking temp: {}'.format(e))
            return True
    command_output = config.temp_function()
    match = None
    if command_output:
        text = str(command_output)
        pattern = re.compile(temp_regex)
        match = re.search(pattern, text)
    if match:
        found_temp = int(match.group(0))
        log.debug('Found temp {}'.format(found_temp))
        if found_temp > stop_temp or found_temp < start_temp:
            return False
    else:
        print('No temps found!')
        log.error('No temps found!')
        return True
    return True


def update_check() -> None:
    """
    Check for updates to the FindTheMag tool
    """
    # If we've checked for updates in the last week, ignore
    delta = datetime.datetime.now() - DATABASE.get('LASTUPDATECHECK', datetime.datetime(1997, 3, 3))
    if abs(delta.days) < 7:
        return
    import requests as req
    headers = req.utils.default_headers()
    headers.update({
        'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
        })

    # Get update status from Github
    url = 'https://raw.githubusercontent.com/makeasnek/FindTheMag2/main/updates.txt'
    try:
        resp = req.get(url, headers = headers).text
    except Exception as e:
        DATABASE['TABLE_STATUS'] = 'Error checking for updates {}'.format(e)
        log.error('Error checking for updates {}'.format(e))
        return
    if 'UPDATE FILE FOR FINDTHEMAG DO NOT DELETE THIS LINE' not in resp:
        DATABASE['TABLE_STATUS'] = 'Error checking for updates invalid update file'
        log.error('Error checking for updates invalid update file')
        return None
    for line in resp.splitlines():
        if line.startswith('#'):
            continue
        if line == '':
            continue
        if ',' not in line:
            continue
        split = line.split(',')
        version = float(split[0])
        if split[1] == '1':
            security = True
        else:
            security = False
        notes = split[2]
        if version > VERSION:
            if security:
                security_text = 'This is an important security update.'
            else:
                security_text = ''
            print(
                'There is an updated version of this tool available ({}). {} Major changes include: {} '.format(
                    version, security_text, notes
                    )
                )
            log.info(
                'There is an updated version of this tool available ({}). {} Major changes include: {} '.format(
                    version, security_text, notes
                    )
                )
    DATABASE['LASTUPDATECHECK'] = datetime.datetime.now()


def get_grc_price() -> float:
    """
    Gets average GRC price from three online sources.
    """
    import requests as req
    found_prices = []
    headers = req.utils.default_headers()
    headers.update({
        'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
        })
    # Get price from coinmarketcap
    url = 'https://coinmarketcap.com/currencies/gridcoin/'
    regex = re.compile('(<div class="priceValue "><span>\$)(\d*\.\d*)(</span>)')
    resp = ''
    try:
        resp = req.get(url, headers = headers).text
    except Exception as e:
        pass
    regex_result = re.search(regex, resp)
    if regex_result:
        answer = float(regex_result.group(2))
        log.info('Found GRC price of {} from coinmarketcap'.format(answer))
        found_prices.append(answer)
    else:
        DATABASE['TABLE_STATUS'] = 'Error getting info from coinmarketcap'
        print('Error getting info from coinmarketcap')

    # Get price from Yahoo
    url = 'https://finance.yahoo.com/quote/GRC-USD/'
    regex = re.compile('(data-field="regularMarketPrice" data-trend="none" data-pricehint="\d" value=")(\d*\.\d*)')
    resp = ''
    try:
        resp = req.get(url, headers = headers).text
    except Exception as e:
        pass
    regex_result = re.search(regex, resp)
    if regex_result:
        answer = float(regex_result.group(2))
        log.info('Found GRC price of {} from Yahoo'.format(answer))
        found_prices.append(answer)
    else:
        DATABASE['TABLE_STATUS'] = 'Error getting info from Yahoo'
        print('Error getting info from Yahoo')

    # Get price from coingecko
    url = 'https://www.coingecko.com/en/coins/gridcoin-research'
    regex = re.compile('(data-coin-symbol="grc" data-target="price.price">\$)(\d*.\d*)', flags = re.MULTILINE | re.IGNORECASE)
    resp = ''
    try:
        resp = req.get(url, headers = headers).text
    except Exception as e:
        pass
    regex_result = re.search(regex, resp)
    if regex_result:
        answer = float(regex_result.group(2))
        log.info('Found GRC price of {} from coingecko'.format(answer))
        found_prices.append(answer)
    else:
        DATABASE['TABLE_STATUS'] = 'Error getting info from coingecko'
        log.error('Error getting info from coingecko')
    # Return average of all found prices
    if len(found_prices) > 0:
        DATABASE['TABLE_STATUS'] = 'Found GRC price {}'.format(sum(found_prices) / len(found_prices))
        return (sum(found_prices) / len(found_prices))
    else:
        DATABASE['TABLE_STATUS'] = 'Unable to find GRC price'
        return 0


def get_approved_project_urls_web() -> Tuple[List[str], Dict[str, str]]:
    """
    Gets current whitelist from Gridcoinstats
    """

    # Return cached version if we have it and requested it < 24 hrs ago
    delta = datetime.datetime.now() - DATABASE.get('LASTGRIDCOINSTATSPROJECTCHECK', datetime.datetime(1993, 3, 3))
    if abs(delta.days) < 1 and 'GSPROJECTLIST' in DATABASE and 'GSRESOLVERDICT' in DATABASE:
        log.debug('Returning cached version of gridcoinstats data')
        return DATABASE['GSPROJECTLIST'], DATABASE['GSRESOLVERDICT']

    # Otherwise, request it
    import json
    url = 'https://www.gridcoinstats.eu/API/simpleQuery.php?q=listprojects'
    import requests as req
    resp = req.get(url)
    if 'BOINC' not in resp.text.upper():
        print('Error fetching magnitude stats from {}'.format(url))
        log.error('Error fetching magnitude stats from {}'.format(url))
        if 'GSPROJECTLIST' in DATABASE and 'GSRESOLVERDICT' in DATABASE:
            log.debug('Returning cached magnitude stats')
            return DATABASE['GSPROJECTLIST'], DATABASE['GSRESOLVERDICT']
        else:
            log.debug('Exiting safely')
            safe_exit(None, None)
    return_list: List[str] = []
    project_resolver_dict: Dict[str, str] = {}
    loaded_json = {}
    try:
        loaded_json = json.loads(resp.text)
    except Exception as e:
        log.error('Error parsing data from Gridcoinstats {}'.format(e))
        if 'GSPROJECTLIST' in DATABASE and 'GSRESOLVERDICT' in DATABASE:
            log.error('Returning old gridcoinstats data'.format(e))
            return DATABASE['GSPROJECTLIST'], DATABASE['GSRESOLVERDICT']
        else:
            print('Unable to continue...')
            safe_exit(None, None)
    for projectname, project in loaded_json.items():
        if 'WORLDCOMMUNITYGRID.ORG/BOINC' in project['base_url'].upper():
            return_list.append(project['base_url'].upper().replace('/BOINC/', ''))
        else:
            return_list.append(project['base_url'].upper())
        project_resolver_dict[projectname] = project['base_url']
    DATABASE['LASTGRIDCOINSTATSPROJECTCHECK'] = datetime.datetime.now()
    DATABASE['GSPROJECTLIST'] = return_list
    DATABASE['GSRESOLVERDICT'] = project_resolver_dict
    return return_list, project_resolver_dict


def wait_till_no_xfers(rpc_client: pyboinc.rpc_client) -> None:
    """
    Wait for BOINC to finish all pending xfers, return None when done
    """
    max_loops = 30
    current_loops = 0
    loop_wait_in_seconds = 30  # wait this long between loops

    def xfers_happening(xfer_list: list) -> bool:
        """
        Returns True if any active xfers are happening, false if none are happening or if only stalled xfers exist
        """
        # Known statuses:
        # 0 = Active
        if isinstance(xfer_list, str):
            return False
        for xfer in xfer_list:
            if str(xfer['status']) == '0':
                if 'persistent_file_xfer' in xfer:
                    if float(xfer['persistent_file_xfer'].get('num_retries', 0)) > 1:
                        continue  # assume xfers with multiple retries are stalled
                return True
            else:
                log.warning('Found xfer with unknown status: ' + str(xfer))
        return False

    # Every ten seconds we will request the list of file transfers from BOINC until there are none left
    while current_loops < max_loops:
        current_loops += 1
        # Ask BOINC for a list of file transfers
        allow_response = loop.run_until_complete(run_rpc_command(rpc_client, 'get_file_transfers'))
        cleaned_response = ''
        if xfers_happening(allow_response):
            log.debug('xfers happening: {}'.format(str(allow_response)))
            sleep(loop_wait_in_seconds)
            continue
        # Remove whitespace etc
        if isinstance(allow_response, list):
            log.error('Unexpected response1 in wait_till_no_xfers: ' + str(allow_response))
        elif isinstance(allow_response, str):
            cleaned_response = allow_response.replace(' ', '')
            cleaned_response = cleaned_response.replace('\n', '')
            if cleaned_response == '':  # There are no transfers, yay!
                return
            else:
                log.error('Unexpected response2 in wait_till_no_xfers: ' + str(cleaned_response))
        log.error('Unexpected response3 in wait_till_no_xfers: ' + str(cleaned_response))


def get_config_parameters(gridcoin_dir: str) -> Dict[str, str]:
    """
    :param gridcoin_dir: Absolute path to a gridcoin config directory
    :return: All config parameters found, preferring those in the json file to the conf. Note that sidestakes become a list as there may be multiple
    """
    return_dict = dict()
    if 'gridcoinsettings.json' in os.listdir(gridcoin_dir):
        with open(os.path.join(gridcoin_dir, 'gridcoinsettings.json')) as json_file:
            config_dict = json.load(json_file)
            if 'rpcuser' in config_dict:
                return_dict['rpc_user'] = config_dict['rpcuser']
            if 'rpcpass' in config_dict:
                return_dict['rpc_pass'] = config_dict['rpcpass']
            if 'rpcport' in config_dict:
                return_dict['rpc_port'] = config_dict['rpcport']
    if 'gridcoinresearch.conf' in os.listdir(gridcoin_dir):
        with open(os.path.join(gridcoin_dir, 'gridcoinresearch.conf')) as f:
            for line in f:
                if line.startswith('#'):
                    continue
                if line.strip() == "":
                    continue
                try:
                    key = line.split('=')[0]
                    value = line.split('=')[1].replace('\n', '')
                    if '#' in value:
                        value = value.split('#')[0]
                    value = value.strip()
                except Exception as e:
                    log.error('Warning: Error parsing line from config file, ignoring: {} error was {}'.format(line, e))
                    continue
                if key == 'addnode':
                    continue
                if key == 'sidestake':
                    if 'sidestake' not in return_dict:
                        return_dict['sidestake'] = []
                    return_dict['sidestake'].append(value)
                    continue
                if key in return_dict:
                    print(
                        'Warning: multiple values found for ' + key + ' in gridcoin config file at '
                        + os.path.join(gridcoin_dir, 'gridcoinresearch.conf') + ' using the first one we found'
                        )
                    log.warning(
                        'Warning: multiple values found for ' + key + ' in gridcoin config file at '
                        + os.path.join(gridcoin_dir, 'gridcoinresearch.conf') + ' using the first one we found'
                        )
                    continue
                return_dict[key] = value
    return return_dict


def check_sidestake(config_params: Dict[str, Union[str, List[str]]], address: str, minval: float) -> bool:
    """
    Checks if a given address is being sidestaked to or not. Returns False if value < minval
    :param config_params: config_params from get_config_parameters
    :param address: address to check
    :param minval: minimum value to pass check
    :return: True or False
    """
    if 'enablesidestaking' not in config_params:
        return False
    if 'sidestake' not in config_params:
        return False
    if config_params['enablesidestaking'] != '1':
        return False
    for sidestake in config_params['sidestake']:
        found_address = sidestake.split(',')[0]
        found_value = float(sidestake.split(',')[1])
        if found_address == address:
            if found_value >= minval:
                return True
    return False


def projecturlfromstatsfile(
    statsfilename: str, all_project_urls: List[str], approved_project_urls: List[str], boinc_projects_list: List[str]
    ) -> str:
    """
    Guess a project url from the name of a stats file
    """
    # remove extraneous information from name
    statsfilename = statsfilename.replace('job_log_', '')
    statsfilename = statsfilename.split('_')[0]
    statsfilename = statsfilename.replace('.txt', '')

    # check if name is in any known URLs
    for knownurl in approved_project_urls:
        if statsfilename.upper() in knownurl:
            return knownurl
    for knownurl in all_project_urls:
        if statsfilename.upper() in knownurl:
            return knownurl
    for knownurl in boinc_projects_list:
        if statsfilename.upper() in knownurl.upper():
            return knownurl.upper()  # we have to upper these as they are not uppered by default
    print(
        'WARNING: Found stats file ' + statsfilename
        + ' but unable to find URL for it, perhaps it is not the BOINC client\'s list of projects?'
        )
    log.warning(
        'WARNING: Found stats file ' + statsfilename
        + ' but unable to find URL for it, perhaps it is not the BOINC client\'s list of projects?'
        )
    return statsfilename


def project_url_from_credit_history_file(
    filename: str, approved_project_urls: List[str], all_project_urls: List[str], boinc_projects_list: List[str]
    ) -> str:
    """
    Guess a project url from credit history file name
    """
    filename = filename.replace('statistics_', '')
    filename = filename.replace('.xml', '')
    filename = filename.split('_')[0]
    for knownurl in approved_project_urls:
        if filename.upper() in knownurl:
            return knownurl
    for knownurl in all_project_urls:
        if filename.upper() in knownurl:
            return knownurl
    for knownurl in boinc_projects_list:
        if filename.upper() in knownurl.upper():
            return knownurl.upper()  # have to upper as this list is not uppered
    print(
        'WARNING: Found credit history file ' + filename
        + ' but unable to find URL for it, perhaps it is not in the BOINC client\'s list of projects?'
        )
    log.error(
        'WARNING: Found credit history file ' + filename
        + ' but unable to find URL for it, perhaps it is not in the BOINC client\'s list of projects?'
        )
    return filename


def stat_file_to_list(stat_file_abs_path: str) -> List[Dict[str, str]]:
    """
        Turns a BOINC job log into list of dicts we can use, each dict is a task. Dicts have keys below:
        STARTTIME,ESTTIME,CPUTIME,ESTIMATEDFLOPS,TASKNAME,WALLTIME,EXITCODE
        Note that ESTIMATEDFLOPS comes from the project and EXITCODE will always be zero.
        All values and keys in dicts are strings.

        BOINC's job log format is:

[ue]	Estimated runtime	BOINC Client estimate (seconds)
[ct]	CPU time		Measured CPU runtime at completion (seconds)
[fe]	Estimated FLOPs count	From project (integer)
[nm]	Task name		From project
[et]	Elapsed time 		Wallclock runtime at completion (seconds)

    """
    stats_list = []
    try:
        with open(stat_file_abs_path, mode = 'r', errors = 'ignore') as f:
            for log_entry in f:
                #log.debug('Found logentry '+str(log_entry))
                match = None
                try:
                    match = re.search(
                        r'(\d*)( ue )([\d\.]*)( ct )([\d\.]*)( fe )(\d*)( nm )(\S*)( et )([\d\.]*)( es )(\d)', log_entry
                        )
                except Exception as e:
                    print(
                        'Error reading BOINC job log at ' + stat_file_abs_path
                        + ' maybe it\'s corrupt? Line: error: '.format(log_entry, e)
                        )
                    log.error(
                        'Error reading BOINC job log at ' + stat_file_abs_path
                        + ' maybe it\'s corrupt? Line: error: '.format(log_entry, e)
                        )
                if not match:
                    print('Encountered log entry in unknown format: ' + log_entry)
                    log.error('Encountered log entry in unknown format: ' + log_entry)
                    continue
                stats = dict()
                stats['STARTTIME'] = match.group(1)
                stats['ESTTIME'] = match.group(3)
                stats['CPUTIME'] = match.group(5)
                stats['ESTIMATEDFLOPS'] = match.group(7)
                stats['TASKNAME'] = match.group(9)
                stats['WALLTIME'] = match.group(11)
                stats['EXITCODE'] = match.group(13)
                stats_list.append(stats)
        return stats_list
    except Exception as e:
        print('Error reading BOINC job log at ' + stat_file_abs_path + ' maybe it\'s corrupt? ' + str(e))
        log.error('Error reading BOINC job log at ' + stat_file_abs_path + ' maybe it\'s corrupt? ' + str(e))
        return []


def resolve_boinc_url_new(url: str):
    '''
    Note: Using resolve_boinc_url_new instead to use get to pass to BOINC, this is for other purposes.
    Given URL, find BOINC's version with appropriate capitalization. If unable to find, print warning and return input
    Prior to a specific BOINC version, RPC calls require capitalization to match identically.
    '''
    cleaned_search_url = url.upper().replace('HTTPS://', '').replace('HTTP://', '').replace('WWW.', '')
    cleaned_search_url = cleaned_search_url.replace('WORLDCOMMUNITYGRID.ORG/BOINC', 'WORLDCOMMUNITYGRID.ORG')
    if cleaned_search_url.endswith('/'):
        cleaned_search_url = cleaned_search_url[:-1]
    for found_url in chain(BOINC_PROJECT_LIST, ALL_BOINC_PROJECTS.keys()):
        cleaned_found_url = found_url.upper().replace('HTTPS://', '').replace('HTTP://', '').replace('WWW.', '')
        if cleaned_search_url == cleaned_found_url or cleaned_search_url in cleaned_found_url:
            return found_url
    return url


def resolve_boinc_url(url: str, boinc_url_list: List[str]):
    '''
    Note: Using resolve_boinc_url_new instead to use get to pass to BOINC, this is for other purposes.
    Given URL, find BOINC's version with appropriate capitalization. If unable to find, print warning and return input
    Prior to a specific BOINC version, RPC calls require capitalization to match identically.
    '''
    cleaned_search_url = url.upper().replace('HTTPS://', '').replace('HTTP://', '').replace('WWW.', '')
    cleaned_search_url = cleaned_search_url.replace('WORLDCOMMUNITYGRID.ORG/BOINC', 'WORLDCOMMUNITYGRID.ORG')
    if cleaned_search_url.endswith('/'):
        cleaned_search_url = cleaned_search_url[:-1]
    for found_url in chain(ALL_BOINC_PROJECTS.keys(), BOINC_PROJECT_LIST):
        cleaned_found_url = found_url.upper().replace('HTTPS://', '').replace('HTTP://', '').replace('WWW.', '')
        if cleaned_search_url == cleaned_found_url or cleaned_search_url in cleaned_found_url:
            return found_url
    return url


async def run_rpc_command(
    rpc_client: pyboinc.rpc_client,
    command: str,
    arg1: Union[str, None] = None,
    arg1_val: Union[str, None] = None,
    arg2: Union[str, None] = None,
    arg2_val: Union[str, None] = None
    ) -> Union[str, Dict[Any, Any]]:
    """
    Runs command on BOINC client via RPC
    Example: run_rpc_command(rpc_client,'project_nomorework','http://project.com/project')
    """
    full_command = '{} {} {} {}'.format(command, arg1, arg1_val, arg2, arg2_val)  # added for debugging purposes
    log.debug('Running BOINC rpc request ' + full_command)
    req = ET.Element(command)
    if arg1 is not None:
        a = ET.SubElement(req, arg1)
        if arg1_val is not None:
            a.text = arg1_val
    if arg2 is not None:
        b = ET.SubElement(req, arg2)
        if arg2_val is not None:
            b.text = arg2_val
    response = await rpc_client._request(req)
    parsed = parse_generic(response)
    if not str(parsed):
        print('Warning: Error w RPC command {}: {}'.format(full_command, parsed))
        log.error('Warning: Error w RPC command {}: {}'.format(full_command, parsed))
    return parsed


def credit_history_file_to_list(credithistoryfileabspath: str) -> List[Dict[str, str]]:
    """
    Turns a BOINC credit history file into list of dicts we can use. Dicts have keys below:
        TIME,USERTOTALCREDIT,USERRAC,HOSTTOTALCREDIT,HOSTRAC
    Note that ESTIMATEDFLOPS comes from the project and EXITCODE will always be zero.
    """
    statslist = []
    with open(credithistoryfileabspath, mode = 'r', encoding = 'ASCII', errors = 'ignore') as f:
        parsed = xmltodict.parse(f.read())
        for logentry in parsed.get('project_statistics', {}).get('daily_statistics', []):
            stats = {}
            if not isinstance(logentry, dict):
                continue
            stats['TIME'] = logentry['day']
            stats['USERTOTALCREDIT'] = logentry['user_total_credit']
            stats['USERRAC'] = logentry['user_expavg_credit']
            stats['HOSTTOTALCREDIT'] = logentry['host_total_credit']
            stats['HOSTRAC'] = logentry['host_expavg_credit']
            statslist.append(stats)
    return statslist


def config_files_to_stats(config_dir_abs_path: str) -> Dict[str, Dict[str, Union[int, float, Dict[str, Union[float, str]]]]]:
    """
    :param config_dir_abs_path: Absolute path to BOINC data directory
    :return: Dict of stats in format COMBINEDSTATSEXAMPLE in main.py
    """
    stats_files: List[str] = []
    credit_history_files: List[str] = []
    return_stats = {}

    # find files to search through, add them to lists
    for file in os.listdir(config_dir_abs_path):
        if 'job_log' in file:
            stats_files.append(os.path.join(config_dir_abs_path, file))
        if file.startswith('statistics_') and file.endswith('.xml'):
            credit_history_files.append(os.path.join(config_dir_abs_path, file))
    log.debug('Found stats_files: ' + str(stats_files))
    log.debug('Found historical credit info files at: ' + str(credit_history_files))

    # Process stats files
    for statsfile in stats_files:
        project_url = projecturlfromstatsfile(
            os.path.basename(statsfile),
            ALL_PROJECT_URLS,
            approved_project_urls = APPROVED_PROJECT_URLS,
            boinc_projects_list = BOINC_PROJECT_LIST
            )
        stat_list = stat_file_to_list(statsfile)
        log.debug('In statsfile for ' + project_url)
        # Compute the first and last date in the stats file. Currently not used but does work
        startdate = str(datetime.datetime.fromtimestamp(float(stat_list[0]['STARTTIME'])).strftime('%m-%d-%Y'))
        lastdate = str(datetime.datetime.fromtimestamp(float(stat_list[len(stat_list) - 1]['STARTTIME'])).strftime('%m-%d-%Y'))
        log.debug('Start date is ' + startdate)
        if project_url not in return_stats:
            return_stats[project_url] = {'CREDIT_HISTORY': {}, 'WU_HISTORY': {}, 'COMPILED_STATS': {}}
        wu_history = return_stats[project_url]['WU_HISTORY']
        for wu in stat_list:
            date = str(datetime.datetime.fromtimestamp(float(wu['STARTTIME'])).strftime('%m-%d-%Y'))
            if date not in wu_history:
                wu_history[date] = {'TOTALWUS': 0, 'total_wall_time': 0, 'total_cpu_time': 0}
            wu_history[date]['TOTALWUS'] += 1
            wu_history[date]['total_wall_time'] += float(wu['WALLTIME'])
            wu_history[date]['total_cpu_time'] += float(wu['CPUTIME'])
    # process credit logs
    for credit_history_file in credit_history_files:
        project_url = project_url_from_credit_history_file(
            os.path.basename(credit_history_file),
            APPROVED_PROJECT_URLS,
            ALL_PROJECT_URLS,
            boinc_projects_list = BOINC_PROJECT_LIST
            )
        credithistorylist = credit_history_file_to_list(credit_history_file)
        if len(credithistorylist) > 0:
            # print('In credit_history_file for ' + project_url)
            startdate = str(datetime.datetime.fromtimestamp(float(credithistorylist[0]['TIME'])).strftime('%m-%d-%Y'))
            lastdate = str(
                datetime.datetime.fromtimestamp(float(credithistorylist[len(credithistorylist) - 1]['TIME'])
                                               ).strftime('%m-%d-%Y')
                )
        for index, entry in enumerate(credithistorylist):
            if index == len(credithistorylist) - 1:  # Skip the last entry as it's already calculated at the previous entry
                continue
            next_entry = credithistorylist[index + 1]
            current_time = float(entry['TIME'])
            delta_credits = float(next_entry['HOSTTOTALCREDIT']) - float(entry['HOSTTOTALCREDIT'])
            # Add found info to combined average stats
            date = str(datetime.datetime.fromtimestamp(float(current_time)).strftime('%m-%d-%Y'))
            if project_url not in return_stats:
                return_stats[project_url] = {'CREDIT_HISTORY': {}, 'WU_HISTORY': {}, 'COMPILED_STATS': {}}
            if 'CREDIT_HISTORY' not in return_stats[project_url]:
                return_stats[project_url]['CREDIT_HISTORY'] = {}
            credit_history = return_stats[project_url]['CREDIT_HISTORY']
            if 'COMPILED STATS' not in return_stats[project_url]:
                return_stats[project_url]['COMPILED_STATS'] = {}
            if date not in credit_history:
                credit_history[date] = {}
            if 'CREDITAWARDED' not in credit_history[date]:
                credit_history[date]["CREDITAWARDED"] = 0
            credit_history[date]['CREDITAWARDED'] += delta_credits
    # find averages
    for project_url, parent_dict in return_stats.items():
        total_wus = 0
        total_credit = 0
        total_cpu_time = 0
        total_wall_time = 0
        x_day_wall_time = 0
        for date, credit_history in parent_dict['CREDIT_HISTORY'].items():
            total_credit += credit_history['CREDITAWARDED']
        for date, wu_history in parent_dict['WU_HISTORY'].items():
            total_wus += wu_history['TOTALWUS']
            total_wall_time += wu_history['total_wall_time']
            split_date = date.split('-')
            datetimed_date = datetime.datetime(year = int(split_date[2]), month = int(split_date[0]), day = int(split_date[1]))
            time_ago = datetime.datetime.now() - datetimed_date
            days_ago = time_ago.days
            if days_ago <= rolling_weight_window:
                x_day_wall_time += wu_history['total_wall_time']
            total_cpu_time += wu_history['total_cpu_time']
        if total_wus == 0:
            avg_wall_time = 0
            avg_cpu_time = 0
            avg_credit_per_task = 0
            credits_per_hour = 0
        else:
            total_cpu_time = total_cpu_time / 60 / 60  # convert to hours
            total_wall_time = total_wall_time / 60 / 60  #convert to hours
            x_day_wall_time = x_day_wall_time / 60 / 60  # convert to hours
            avg_wall_time = total_wall_time / total_wus
            avg_cpu_time = total_cpu_time / total_wus
            avg_credit_per_task = total_credit / total_wus
            credits_per_hour = (total_credit / (total_wall_time))
        parent_dict['COMPILED_STATS']['TOTALCREDIT'] = total_credit
        parent_dict['COMPILED_STATS']['AVGWALLTIME'] = avg_wall_time
        parent_dict['COMPILED_STATS']['AVGCPUTIME'] = avg_cpu_time
        parent_dict['COMPILED_STATS']['AVGCREDITPERTASK'] = avg_credit_per_task
        parent_dict['COMPILED_STATS']['TOTALTASKS'] = total_wus
        parent_dict['COMPILED_STATS']['TOTALWALLTIME'] = total_wall_time
        parent_dict['COMPILED_STATS']['TOTALCPUTIME'] = total_cpu_time
        parent_dict['COMPILED_STATS']['AVGCREDITPERHOUR'] = credits_per_hour
        parent_dict['COMPILED_STATS']['XDAYWALLTIME'] = x_day_wall_time
        log.debug(
            'For project {} this host has crunched {} WUs for {} total credit with an average of {} credits per WU. {} hours were spent on these WUs for {} credit/hr'
            .format(
                project_url.lower(), total_wus, round(total_credit, 2), round(avg_credit_per_task, 2),
                round((total_wall_time), 2), round(credits_per_hour, 2)
                )
            )
    return return_stats


def add_mag_to_combined_stats(
    combined_stats: dict, mag_ratios: Dict[str, float], approved_projects: List[str], preferred_projects: List[str]
    ) -> Tuple[dict, List[str]]:
    """
    :param combined_stats: combined_stats from main.py
    :param mag_ratios: mag ratios returned from get_project_mag_ratios. A dict with project URL as key and mag ratio as value
    :return: combined_stats w/ mag ratios added to us, list of projects which are being crunched but not on approved projects list
    """
    unapproved_list = []
    for project_url, project_stats in combined_stats.items():
        found_mag_ratio = get_project_from_dict(project_url, mag_ratios, 'searching mag_ratios')
        if not found_mag_ratio:
            if project_url not in approved_projects:
                if project_url not in preferred_projects:
                    unapproved_list.append(project_url.lower())
            project_stats['COMPILED_STATS']['AVGMAGPERHOUR'] = 0
            project_stats['COMPILED_STATS']['MAGPERCREDIT'] = 0
            continue
        avg_credit_per_hour = 0
        if 'AVGCREDITPERHOUR' in project_stats['COMPILED_STATS']:
            avg_credit_per_hour = project_stats['COMPILED_STATS']['AVGCREDITPERHOUR']
        project_stats['COMPILED_STATS']['AVGMAGPERHOUR'] = avg_credit_per_hour * found_mag_ratio
        project_stats['COMPILED_STATS']['MAGPERCREDIT'] = found_mag_ratio
    return combined_stats, unapproved_list


def get_most_mag_efficient_projects(
    combinedstats: dict, ignored_projects: List[str], percentdiff: int = 10, quiet: bool = False
    ) -> List[str]:
    """
    Given combinedstats, return most mag efficient project(s). This is the #1 most efficient project and any other projects which are within percentdiff of that number.
    :param combinedstats: combinedstats dict
    :param percentdiff: Maximum percent diff
    :return: List of project URLs
    """

    def is_eligible(project_url: str, project_stats: dict):
        # Ignore projects and projects w less than 10 completed tasks are ineligible
        if project_url in ignored_projects:
            return False
        if int(project_stats['COMPILED_STATS']['TOTALTASKS']) >= 10:
            return True
        return False

    return_list = []
    highest_project = None
    try:
        highest_project = next(iter(combinedstats))  # first project is the "highest project" until we test others against it
    except Exception as e:
        if not quiet:
            print(
                'Searching for most mag efficient projects.. No projects found? Assuming this is a brand new BOINC install'
                + str(e)
                )
        log.error(
            'Searching for most mag efficient projects.. No projects found? Assuming this is a brand new BOINC install'
            + str(e)
            )
        return []

    # find the highest project
    for project_url, project_stats in combinedstats.items():
        current_mag_per_hour = project_stats['COMPILED_STATS']['AVGMAGPERHOUR']
        highest_mag_per_hour = combinedstats[highest_project]['COMPILED_STATS']['AVGMAGPERHOUR']
        if current_mag_per_hour > highest_mag_per_hour and is_eligible(project_url, project_stats):
            highest_project = project_url
    if combinedstats[highest_project]['COMPILED_STATS']['TOTALTASKS'] >= 10:
        if not quiet:
            print(
                '\n\nHighest mag/hr project --with at least 10 completed WUs-- is {} w/ {}/hr of credit.'.format(
                    highest_project.lower(), combinedstats[highest_project]['COMPILED_STATS']['AVGMAGPERHOUR']
                    )
                )
        log.info(
            '\n\nHighest mag/hr project //with at least 10 completed WUs// is {} w/ {}/hr of credit.'.format(
                highest_project.lower(), combinedstats[highest_project]['COMPILED_STATS']['AVGMAGPERHOUR']
                )
            )
    return_list.append(highest_project)

    # then compare other projects to it to see if any are within 10% of it
    highest_avg_mag = combinedstats[highest_project]['COMPILED_STATS']['AVGMAGPERHOUR']
    minimum_for_inclusion = highest_avg_mag - (highest_avg_mag * .10)
    for project_url, project_stats in combinedstats.items():
        current_avg_mag = project_stats['COMPILED_STATS']['AVGMAGPERHOUR']
        if project_url == highest_project:
            continue
        if minimum_for_inclusion <= current_avg_mag and is_eligible(project_url, project_stats) and current_avg_mag != 0:
            if not quiet:
                print(
                    'Also including this project because it\'s within 10% variance of highest mag/hr project: {}, mag/hr {}'
                    .format(project_url.lower(), current_avg_mag)
                    )
            log.info(
                'Also including this project because it\'s within 10% variance of highest mag/hr project: {}, mag/hr {}'.format(
                    project_url.lower(), current_avg_mag
                    )
                )
            return_list.append(project_url)

    #If there is no highest project, return empty list
    if len(return_list) == 1:
        if combinedstats[highest_project]['COMPILED_STATS']['TOTALTASKS'] < 10:
            return_list.clear()
    return return_list


def sidestake_check(check_sidestake_results: bool, check_type: str, address: str) -> None:
    if check_type == 'FOUNDATION':
        message1 = 'It appears that you have not enabled sidestaking to the Gridcoin foundation in your wallet. We believe it is only fair that people benefiting from the Gridcoin network contribute back to it\nSidestaking enables you to contribute a small % of your staking profits (you can choose the %)\nWould you like to enable sidestaking?. \nPlease answer "Y" or "N" (without quotes)'
        message2 = 'What percent would you like to donate to the Gridcoin foundation? Donations go towards software development, promotion, and growth of the coin. Enter a number like 5 for 5%. Please enter whole numbers only'
    elif check_type == 'DEVELOPER':
        message1 = 'Are you interested in sidestaking to the developers of this tool? This is optional. We ask you to consider what gain in efficiency this tool can bring you and to donate a small portion of that gain (you can choose the %).\nPlease. I am trying to buy a pony.\n Please answer "Y" or "N" (without quotes)'
        message2 = 'What percent would you like to donate to the developers of this tool? Enter a number like 5 for 5%. Please enter whole numbers only'
    else:
        message1 = ''
        message2 = ''
    if not check_sidestake_results:
        answer = input(message1)
        while answer not in ['Y', 'N']:
            print('Error: Y or N not entered. Try again please :)')
            answer = input("")
        if answer == 'N':
            if check_type == 'FOUNDATION':
                print('Ok no problem, it is your choice after all!')
            if check_type == 'DEVELOPER':
                print('Ok no problem, it is your choice after all!')
                return
        print(message2)
        answer = input("")
        converted_value = None
        while not converted_value:
            try:
                converted_value = int(answer)
            except Exception as e:
                print("Hmm... that didn't seem to work, let's try again. Please enter a whole number")
                answer = input("")
        with open(os.path.join(gridcoin_data_dir, 'gridcoinresearch.conf'), "a") as myfile:
            if 'enablesidestaking=1' not in str(myfile):
                myfile.write("enablesidestaking=1\n")
            myfile.write('sidestake=' + address + ',' + str(converted_value) + '\n')


def get_project_mag_ratios(grc_client: GridcoinClientConnection, lookback_period: int = 30) -> Dict[str, float]:
    """
    :param grc_client:
    :param lookback_period: number of superblocks to look back to determine average
    :return: Dictionary w/ key as project URL and value as project mag ratio (mag per unit of RAC)
    """
    projects = {}
    return_dict = {}
    mag_per_project = 0
    command_result = grc_client.run_command('superblocks', [30, True])
    for i in range(0, lookback_period):
        superblock = command_result['result'][i]
        if i == 0:
            total_magnitude = superblock['total_magnitude']
            total_projects = superblock['total_projects']
            mag_per_project = total_magnitude / total_projects
        for project_name, project_stats in superblock['Contract Contents']['projects'].items():
            if project_name not in projects:
                if i == 0:
                    projects[project_name] = []
                else:
                    continue  # skip projects which are on greylist
            projects[project_name].append(project_stats['rac'])
    for project_name, project_racs in projects.items():
        average_rac = sum(project_racs) / len(project_racs)
        project_url = grc_client.project_name_to_url(project_name)
        return_dict[project_url] = mag_per_project / average_rac
    return return_dict


def project_url_to_name(url: str, project_names: dict = None):
    if not project_names:
        project_names = BOINC_PROJECT_NAMES
    search = url.lower().replace('https://', '').replace('http://', '').replace('www.', '')
    found = search
    for project_url, name in project_names.items():
        if search in project_url.lower():
            found = name.lower().replace('@home', '').replace('athome', '')
    return found


def print_table(
    table_dict: Dict[str, Dict[str, str]],
    sortby: str = 'GRC/HR',
    sleep_reason: str = DATABASE['TABLE_SLEEP_REASON'],
    status: str = DATABASE['TABLE_STATUS']
    ):

    def left_align(yourstring: str, total_len: int, min_pad: int = 0) -> str:
        """
        Return left-aligned string with a total len of X and min_padding (extra space on right side) of min_pad, cutting off string if needed
        If min_pad==1, it looks like this ' yourstring '
        """
        if len(yourstring) >= total_len - min_pad:
            yourstring = yourstring[0 : total_len - (min_pad)]
        space_left = total_len - (len(yourstring) + min_pad)
        right_pad = ' ' * space_left
        return yourstring + right_pad

    def center_align(yourstring: str, total_len: int, min_pad: int = 0) -> str:
        """
        Return center-aligned string with a total len of X and min_padding (extra space on right & left side) of min_pad, cutting off string if needed
        If min_pad==1, it looks like this ' yourstring '
        """
        if len(yourstring) >= total_len - min_pad:
            yourstring = yourstring[0 : total_len - (min_pad)]
        space_left = total_len - (len(yourstring) + min_pad)
        left_pad = ' ' * floor(space_left / 2)
        right_pad = ' ' * ceil(space_left / 2)
        return left_pad + yourstring + right_pad

    if len(table_dict) == 0:
        return
    headings = []
    heading_length: Dict[str, int] = {}  # length of each heading column
    values = {}
    working_dict = copy.deepcopy(table_dict)
    # convert urls to nice names, add USD/GRC/hr
    for url in list(working_dict.keys()):
        name = project_url_to_name(url, ALL_BOINC_PROJECTS)
        if not name:
            name = url
        stats = table_dict[url]
        working_dict[name] = stats
        if name != url:
            del working_dict[url]
        # add usd/grc/hr to each project
        if working_dict[name].get('MAG/HR'):
            grc_per_hour = float(working_dict[name].get('MAG/HR', 0)) / 4
            grc_per_day = (float(working_dict[name].get('MAG/HR', 0)) / 4) * 24
            working_dict[name]['GRC/HR'] = str('{:.3f}').format(grc_per_hour)
            working_dict[name]['GRC/DAY'] = str('{:.3f}').format(grc_per_day)
            if float(working_dict[name].get('MAG/HR')) != 0:
                revenue_per_hour = (float(working_dict[name].get('MAG/HR')) / 4) * DATABASE.get('GRCPRICE', 0)
                exchange_expenses = revenue_per_hour * exchange_fee
                expenses_per_hour = exchange_expenses + HOST_COST_PER_HOUR
                profit = revenue_per_hour - expenses_per_hour
                working_dict[name]['USD/HR R/P'] = '{:.4f}/{:.4f}'.format(revenue_per_hour, profit)
            else:
                working_dict[name]['USD/HR R/P'] = '0'
            del working_dict[name]['MAG/HR']

    # figure out table headings
    for url, stats in working_dict.items():
        for key, value in stats.items():
            if key not in headings:
                headings.append(key)
            if key not in heading_length:
                heading_length[key] = len(key) + 2
            heading_length[key] = len(key) + 2
            if key not in values:
                values[key] = []
            if value not in values[key]:
                values[key].append(value)

    longest_url = len(max(working_dict.keys(), key = len))
    table_width = longest_url + len(str(values.keys()))
    # print header
    ## print first line
    print('*' * table_width)
    print('*' + center_align('FINDTHEMAG V2.0', table_width - 2) + '*')
    print('*' * table_width)

    ## print rest of header
    padding_str = ' ' * (longest_url + 1)
    print('*' + padding_str, end = '|')
    for heading in headings:
        print(center_align(heading, heading_length[heading]) + '|', end = "")
    print('')

    # print contents
    sortedprojects = sorted(working_dict.keys(), key = lambda a: float(working_dict[a].get(sortby, 0)), reverse = True)
    for url in sortedprojects:
        stats = working_dict[url]
        url_padding = longest_url - len(url)
        url_padding_str = ' ' * url_padding
        print('* ' + url.lower() + url_padding_str, end = '|')
        for heading in headings:
            value = stats.get(heading, '')
            print(left_align(value, heading_length[heading]), end = "|")
        print('')

    # print bottom bar
    print('*' * table_width)
    if not sleep_reason:
        sleep_reason = 'NONE'
    elif sleep_reason == '':
        sleep_reason = 'NONE'
    bottom_bar_1 = '*' + left_align('Sleep reason: {}'.format(sleep_reason), total_len = 60, min_pad = 1) + '*'
    bottom_bar_2 = left_align('Info: {}'.format(status), total_len = 60, min_pad = 1)
    bottom_bar_3 = left_align('GRC Price: {:.4f}'.format(DATABASE.get('GRCPRICE', 0.00000)), total_len = 17, min_pad = 1) + '*'
    print(bottom_bar_1 + bottom_bar_2 + bottom_bar_3)
    # print improved stats
    addl = ''
    curr_avg_mag = get_avg_mag_hr(combined_stats)
    if curr_avg_mag > DATABASE['STARTMAGHR'] and DATABASE['STARTMAGHR'] > 0:
        increase = (curr_avg_mag - DATABASE['STARTMAGHR']) / DATABASE['STARTMAGHR']
        addl = " That's an increase of {:.2f}%!".format(increase)
    print(
        'When you started using this tool, your average mag/hr was: {:.4f} now it is {:.4f}'
        .format(DATABASE['STARTMAGHR'], get_avg_mag_hr(combined_stats)) + addl
        )
    print('Hours crunched: {:.1f} '.format(DATABASE['FTMTOTAL'] / 60))
    # print final line
    if not check_sidestake_results:
        print('Consider donating to this app\'s development directly or via sidestake: RzUgcntbFm8PeSJpauk6a44qbtu92dpw3K.')
    print('Use Ctrl+C to exit FTM and return BOINC to previous config')
    print('*' * table_width)


def in_list(str, list) -> bool:
    search_str = str.upper()
    search_str = search_str.replace('HTTPS://', '')
    search_str = search_str.replace('HTTP://', '')
    search_str = search_str.replace('WWW.', '')
    search_str = search_str.replace('WORLDCOMMUNITYGRID.ORG/BOINC/', 'WORLDCOMMUNITYGRID.ORG')  # fix for WCG
    for item in list:
        if search_str == item.upper() or search_str in item.upper():
            return True
    return False


def generate_stats(
    APPROVED_PROJECT_URLS: List[str],
    preferred_projects: Dict[str, float] = preferred_projects,
    ignored_projects: List[str] = ignored_projects,
    quiet: bool = False,
    ignore_unattached: bool = False,
    attached_list: List[str] = None,
    mag_ratios = Dict[str, float]
    ):
    if not attached_list:
        attached_list = []
    weak_stats = []
    if not quiet:
        print('Gathering project stats...')
        log.info('Gathering project stats..')
    combined_stats = config_files_to_stats(boinc_data_dir)
    if not quiet:
        print_and_log('Calculating project weights...', 'INFO')
        print('Curing some cancer along the way...')
    # Calculate project weights w/ credit/hr
    final_project_weights = {}
    # Uppercase preferred_projects list
    for url in list(preferred_projects.keys()):
        weight = preferred_projects[url]
        del preferred_projects[url]
        preferred_projects[url.upper()] = weight
    ignored_projects = [x.upper() for x in ignored_projects]  # uppercase ignored project url list
    # ignore unattached projects if requested
    if ignore_unattached:
        for project in APPROVED_PROJECT_URLS:
            if not in_list(project, attached_list):
                ignored_projects.append(project.upper())
                log.warning('Ignoring whitelisted project {} bc not attached'.format(project))
    combined_stats, unapproved_projects = add_mag_to_combined_stats(
        combined_stats, mag_ratios, APPROVED_PROJECT_URLS, list(preferred_projects.keys())
        )
    if len(unapproved_projects) > 0:
        print(
            'Warning: Projects below were found in your BOINC config but are not on the gridcoin approval list or your preferred projects list. If you want them to be given weight, be sure to add them to your preferred projects'
            )
        log.warning(
            'Warning: Projects below were found in your BOINC config but are not on the gridcoin approval list or your preferred projects list. If you want them to be given weight, be sure to add them to your preferred projects'
            + str(unapproved_projects)
            )
        pprint.pprint(unapproved_projects)
    most_efficient_projects = get_most_mag_efficient_projects(combined_stats, ignored_projects, quiet = quiet)
    if len(most_efficient_projects) == 0:
        print('No projects have enough completed tasks to determine which is the most efficient. Assigning all projects 1')
        log.warning(
            'No projects have enough completed tasks to determine which is the most efficient. Assigning all projects 1'
            )
        total_preferred_weight = 1000 - (len(APPROVED_PROJECT_URLS)) + len(preferred_projects)
        total_mining_weight = 0
    else:
        total_preferred_weight = (preferred_projects_percent / 100) * 1000
        total_mining_weight = 1000 - total_preferred_weight
    total_mining_weight_remaining = total_mining_weight
    # assign weight of 1 to all projects which didn't make the cut
    for project_url in APPROVED_PROJECT_URLS:
        preferred_extract = get_project_from_dict(project_url, preferred_projects, 'IGNOREME')
        if preferred_extract:
            continue  # exclude preferred projects
        if project_url in ignored_projects:
            final_project_weights[project_url] = 0
            continue
        combined_stats_extract = get_project_from_dict(
            project_url, combined_stats, 'searching combined_stats in generate_stats'
            )
        if not combined_stats_extract:
            log.debug('Warning: project has no stats, setting project weight to one: ' + project_url.lower())
            final_project_weights[project_url] = 1
            total_mining_weight_remaining -= 1
            weak_stats.append(project_url.lower())
            continue
        total_tasks = int(combined_stats_extract['COMPILED_STATS']['TOTALTASKS'])
        if total_tasks < 10:
            log.debug(
                'Warning: project does not have enough tasks to compute accurate average, setting project weight to one: '
                + project_url.lower()
                )
            weak_stats.append(project_url.lower())
        if project_url not in most_efficient_projects or total_tasks < 10:
            final_project_weights[project_url] = 1
            total_mining_weight_remaining -= 1
    if len(most_efficient_projects) > 0:
        if quiet:
            log.debug(
                'The following projects do not have enough stats to be calculated accurately, assigning them a weight of one: '
                + str(weak_stats)
                )
        else:
            print_and_log(
                'The following projects do not have enough stats to be calculated accurately, assigning them a weight of one: ',
                'INFO'
                )
            pprint.pprint(weak_stats)
    # Figure out weight to assign to most efficient projects, assign it
    if len(most_efficient_projects) == 0:
        per_efficient_project = 0
    else:
        per_efficient_project = total_mining_weight_remaining / len(most_efficient_projects)
    if total_mining_weight_remaining > 0:
        if not quiet:
            print(
                'Assigning ' + str(total_mining_weight_remaining) + ' weight to ' + str(len(most_efficient_projects))
                + ' mining projects which means ' + str(per_efficient_project) + ' per project '
                )
            log.info(
                'Assigning ' + str(total_mining_weight_remaining) + ' weight to ' + str(len(most_efficient_projects))
                + ' mining projects which means ' + str(per_efficient_project) + ' per project '
                )
    for project_url in most_efficient_projects:
        if project_url not in final_project_weights:
            final_project_weights[project_url] = 0
        final_project_weights[project_url] += per_efficient_project
    # Assign weight to preferred projects
    for project_url, weight in preferred_projects.items():
        final_project_weights_extract = get_project_from_dict(project_url, final_project_weights, 'IGNOREME')
        preferred_project_weights_extract = get_project_from_dict(
            project_url, preferred_projects, 'searching preferred_projects in generate_stats'
            )
        if not final_project_weights_extract:
            final_project_weights[project_url] = 0
        intended_weight = (preferred_project_weights_extract / 100) * total_preferred_weight
        final_project_weights[project_url] += intended_weight
    return combined_stats, final_project_weights, total_preferred_weight, total_mining_weight


async def kill_all_unstarted_tasks(rpc_client: pyboinc.rpc_client, task_list: list):
    project_status_reply = await rpc_client.get_project_status()
    found_projects = []  # DEBUG ADDED TYPE THIS CORRECTLY
    for task in task_list:
        #elapsed_time=task['active_task']['current_cpu_time'].seconds
        name = task['name']
        wu_name = task['wu_name']
        project_url = task['project_url'].master_url
        if 'active_task' not in task:
            print('Cancelling unstarted task {}'.format(task))
            log.info('Cancelling unstarted task {}'.format(task))
            req = ET.Element('abort_result')
            a = ET.SubElement(req, 'project_url')
            a.text = project_url
            b = ET.SubElement(req, 'name')
            b.text = name
            response = await rpc_client._request(req)
            parsed = parse_generic(response)  # returns True if successful
            a = "21"
        else:
            #print('Keeping task {}'.format(task))
            log.debug('Keeping task {}'.format(task))


async def nnt_all_projects(rpc_client: pyboinc.rpc_client):
    project_status_reply = await rpc_client.get_project_status()
    found_projects = []
    for project in project_status_reply:
        found_projects.append(project.master_url)
    for project in found_projects:
        req = ET.Element('project_nomorework')
        a = ET.SubElement(req, 'project_url')
        a.text = project
        response = await rpc_client._request(req)
        parsed = parse_generic(response)  # returns True if successful


async def check_log_entries(rpc_client: pyboinc.rpc_client, project_name: str) -> bool:
    """
    Return True if project cache full, False otherwise.
    project_name: name of project as it will appear in BOINC logs, NOT URL
    """

    def ignore_message(message):
        ignore_phrases = [
            'work fetch resumed by user', 'update requested by user', 'sending scheduler request',
            'scheduler request completed', 'project requested delay', 'work fetch suspended by user', 'Started download of',
            'Finished download of', 'Starting task', 'Requesting new tasks'
            'last request too recent'
            'master file download succeeded', 'No tasks sent', 'Requesting new tasks for', 'no tasks are available for',
            'computation for task', 'started upload of', 'finished upload of',
            'This computer has reached a limit on tasks in progress',
            'Upgrade to the latest driver to process tasks using your computer\'s GPU', 'project has no tasks available'
            ]
        uppered_message = str(message).upper()
        for phrase in ignore_phrases:
            if phrase.upper() in uppered_message:
                return True
        if 'UP TO' in uppered_message and 'NEEDS' in uppered_message and 'IS AVAILABLE FOR USE' in uppered_message and 'BUT ONLY' in uppered_message:
            return True
        if 'REPORTING' and 'COMPLETED TASKS' in uppered_message:
            return True
        return False

    def cache_full(project_name: str, messages) -> bool:
        """
        Returns TRUE if CPU /AND/ GPU cache full, False is either is un-full.
        Systems w/o GPU will be assumed to have a "full cache" for GPU
        """
        cpu_full = False
        gpu_full = False
        for message in messages:
            if project_name.upper() not in str(message).upper():
                continue
            difference = datetime.datetime.now() - message['time']
            if difference.seconds > 60 * 5:  # if message is > 5 min old, skip
                continue
            if project_name.upper() == message['project'].upper():
                if "Not requesting tasks: don't need".upper() in message['body'].upper():
                    if 'GPU' not in message['body'].upper():
                        gpu_full = True  # if no GPU, GPU cache is always full
                    if "CPU: job cache full".upper() in message['body'].upper(
                    ) or "Not requesting tasks: don't need (job cache full)".upper() in message['body'].upper():
                        cpu_full = True
                        #print('CPU cache appears full {}'.format(message['body']))
                        log.debug('CPU cache appears full {}'.format(message['body']))
                    else:
                        if "Not requesting tasks: don't need ()".upper() in message['body'].upper():
                            pass
                        else:
                            #print('CPU cache appears not full {}'.format(message['body']))
                            log.debug('CPU cache appears not full {}'.format(message['body']))
                    if "GPU: job cache full".upper() in message['body'].upper():
                        gpu_full = True
                        #print('GPU cache appears full {}'.format(message['body']))
                        log.debug('GPU cache appears full {}'.format(message['body']))
                    elif 'GPUs not usable'.upper() in message['body'].upper():
                        gpu_full = True
                        log.debug('GPU cache appears full {}'.format(message['body']))
                    else:
                        if "Not requesting tasks: don't need ()".upper() in message['body'].upper():
                            pass
                        else:
                            if not gpu_full:  # if GPU is not mentioned in log, this would always happen so using this to stop erroneous messages
                                #print('GPU cache appears not full {}'.format(message['body']))
                                log.debug('GPU cache appears not full {}'.format(message['body']))
                    continue
                elif ignore_message(message):
                    pass
                else:
                    log.warning('Found unknown message1: {}'.format(message['body']))
        if cpu_full and gpu_full:
            return True
        return False

    # Get message count
    req = ET.Element('get_message_count')
    msg_count_response = await rpc_client._request(req)
    message_count = int(parse_generic(msg_count_response))
    req = ET.Element('get_messages')
    a = ET.SubElement(req, 'seqno')
    a.text = str(message_count - 50)  # get ten most recent messages
    messages_response = await rpc_client._request(req)
    messages = parse_generic(messages_response)  # returns True if successful
    if cache_full(project_name, messages):
        return True
    return False


async def check_log_entries_for_backoff(rpc_client: pyboinc.rpc_client, project_name: str) -> bool:
    """
    Return True if project should be backed off, False otherwise
    project_name: name of project as it will appear in BOINC logs, NOT URL
    """

    def ignore_message(message, ignore_phrases: List[str]):
        lowered = str(message['body']).lower()
        uppered = str(message['body']).upper()
        for phrase in ignore_phrases:
            if phrase.upper() in uppered:
                return True
        if 'got' and 'new tasks' in lowered:
            return True
        if 'reporting' and 'completed tasks' in lowered:
            return True
        if 'computation for task' and 'finished' in lowered:
            return True
        return False

    def project_backoff(project_name: str, messages) -> bool:
        """
        Returns TRUE if project should be backed off. False otherwise or if unable to determine
        """
        #Phrases which indicate project SHOULD be backed off
        # removed 'project requested delay' from positive phrases because projects always provide this, even if work was provided!
        positive_phrases = [
            'project has no tasks available', 'scheduler request failed', 'no tasks sent', 'last request too recent',
            'An NVIDIA GPU is required to run tasks for this project'
            ]
        # Phrases which indicate project SHOULD NOT be backed off
        negative_phrases = ["Not requesting tasks: don't need", 'started download', 'Finished download of']
        # Phrases which indicate we can skip this log entry
        ignore_phrases = [
            'work fetch resumed by user', 'update requested by user', 'work fetch suspended by user', 'Starting task',
            'Requesting new tasks', 'sending scheduler request', 'scheduler request completed', 'started upload',
            'finished upload', 'master file download succeeded', 'fetching scheduler list',
            'Upgrade to the latest driver to process tasks using your computer\'s GPU', 'not started and deadline has passed',
            'Project requested delay of'
            ]
        for message in messages:
            uppered_body = message['body'].upper()
            if project_name.upper() not in str(message).upper():
                continue
            difference = datetime.datetime.now() - message['time']
            if difference.seconds > 60 * 5:  # if message is > 5 min old, skip
                continue
            if ignore_message(message, ignore_phrases):
                continue
            for phrase in positive_phrases:
                if phrase.upper() in uppered_body:
                    log.debug('Backing off {} bc {} in logs'.format(project_name, phrase))
                    return True
            for phrase in negative_phrases:
                if phrase.upper() in uppered_body:
                    return False
            if 'NEEDS' in uppered_body and 'BUT ONLY' in uppered_body and 'IS AVAILABLE FOR USE' in uppered_body:
                log.debug('Backing off {} bc NEEDS BUT ONLY AVAILABLE FOR USE in logs'.format(project_name), 'DEBUG')
                return True
            log.warning('Found unknown messagex: {}'.format(message['body']))
        log.warning('Unable to determine if project {} should be backed off, assuming no'.format(project_name))
        return False

    # Get message count
    req = ET.Element('get_message_count')
    msg_count_response = await rpc_client._request(req)
    message_count = int(parse_generic(msg_count_response))
    req = ET.Element('get_messages')
    a = ET.SubElement(req, 'seqno')
    a.text = str(message_count - 50)  # get ten most recent messages
    messages_response = await rpc_client._request(req)
    messages = parse_generic(messages_response)  # returns True if successful
    if project_name.upper() == 'GPUGRID.NET':
        project_name = 'GPUGRID'  # fix for log entries which show up under different name
    return project_backoff(project_name, messages)


async def get_all_projects(rpc_client: pyboinc.rpc_client) -> Dict[str, str]:
    """
    Get ALL projects the BOINC client knows about, even if unattached
    """
    req = ET.Element('get_all_projects_list')
    messages_response = await rpc_client._request(req)
    project_status_reply = parse_generic(messages_response)  # returns True if successful
    found_projects = []
    project_names = {}
    for project in project_status_reply:
        project_names[project['url']] = project['name']
    project_names['https://gene.disi.unitn.it/test/'
                 ] = 'TN-Grid'  # added bc BOINC client does not list this project for some reason
    return project_names


async def get_attached_projects(rpc_client: pyboinc.rpc_client) -> Tuple[List[str], Dict[str, str]]:
    project_status_reply = await rpc_client.get_project_status()
    found_projects = []
    project_names = {}
    for project in project_status_reply:
        found_projects.append(project.master_url)
        if isinstance(
            project.project_name, bool
            ):  # this happens if project is "attached" but unable to communicate w project due to it being down or some other issue
            project_names[project.master_url] = project.master_url
        else:
            project_names[project.master_url] = project.project_name
    return found_projects, project_names


async def verify_boinc_connection(rpc_client: pyboinc.rpc_client) -> bool:
    """
    Checks if a BOINC client can be connected to and authorized.
    Returns True if it can, False if it can't.
    """
    authorize_response = await rpc_client.authorize()
    req = ET.Element('get_global_prefs_working')
    response = await rpc_client._request(req)
    if 'unauthorized' in str(response):
        return False
    return True


async def prefs_check(rpc_client: pyboinc.rpc_client) -> dict:
    # authorize BOINC client
    authorize_response = await rpc_client.authorize()
    # get prefs
    req = ET.Element('get_global_prefs_working')
    response = await rpc_client._request(req)
    parsed = parse_generic(response)  # returns True if successful
    # get actual disk usage
    req = ET.Element('get_disk_usage')
    response = await rpc_client._request(req)
    usage = parse_generic(response)  # returns True if successful
    max_gb = int(float(parsed.get('disk_max_used_gb', 0)))
    used_max_gb = int(int(usage['d_allowed']) / 1024 / 1024 / 1024)
    if (max_gb < 10 and max_gb != 0) or used_max_gb < 9.5:
        print(
            "BOINC is configured to use less than 10GB, this tool will not run with <10GB allocated in order to prevent requesting base project files from projects too often."
            )
        log.error(
            "BOINC is configured to use less than 10GB, this tool will not run with <10GB allocated in order to prevent requesting base project files from projects too often."
            )
        print(
            'If you have configured BOINC to be able to use >=10GB and still get this message, it is because you are low on disk space and BOINC is responding to settings such as "don\'t use greater than X% of space" or "leave x% free"'
            )
        log.error(
            'If you have configured BOINC to be able to use >=10GB and still get this message, it is because you are low on disk space and BOINC is responding to settings such as "don\'t use greater than X% of space" or "leave x% free"'
            )
        print('Press enter to quit')
        input()
        quit()
    net_start_hour = int(float(parsed['net_start_hour'])) + int(float(parsed['net_end_hour']))
    if net_start_hour != 0:
        print(
            'You have BOINC configured to only access the network at certain times, this tool requires constant '
            'internet availability.'
            )
        log.error(
            'You have BOINC configured to only access the network at certain times, this tool requires constant '
            'internet availability.'
            )
        print('Press enter to quit')
        input()
    return parsed


def get_highest_priority_project(
    combined_stats: dict,
    project_weights: Dict[str, int],
    min_recheck_time = min_recheck_time,
    attached_projects: List[str] = None,
    quiet: bool = False
    ) -> Tuple[List[str], Dict[str, float]]:
    """
    Given STATS, return list of projects sorted by priority. Note that "benchmark" projects are compared to TOTAL time
    while others are compared to windowed time specific by user
    """
    if not attached_projects:
        attached_projects = []
    priority_dict = {}
    # calculate total time from stats
    total_xday_time = 0
    total_time = 0
    for found_key, projectstats in combined_stats.items():
        total_xday_time += projectstats['COMPILED_STATS']['XDAYWALLTIME']
        total_time += projectstats['COMPILED_STATS']['TOTALWALLTIME']
    #print('Calculating project weights: total time is {}'.format(total_xday_time))
    log.debug('Calculating project weights: total time is {}'.format(total_xday_time))
    for project, weight in project_weights.items():
        if not in_list(project, attached_projects):
            log.debug('skipping project bc not attached {}'.format(project))
            continue
        combined_stats_extract = get_project_from_dict(
            project, combined_stats, 'searching combined_stats in get_highest_priority_project'
            )
        if not combined_stats_extract:
            if not quiet:
                print(
                    'Warning: {} not found in stats, assuming not attached. You can safely ignore this warning w/ a new BOINC install which has not received credit on this project yet '
                    .format(project)
                    )
            log.warning(
                'Warning: {} not found in stats, assuming not attached You can safely ignore this warning w/ a new BOINC install which has not received credit on this project yet '
                .format(project)
                )
            existing_time = 0
        else:
            if weight == 1:  # benchmarking projects should be over ALL time not just recent time
                existing_time = combined_stats_extract['COMPILED_STATS']['TOTALWALLTIME']
            else:
                existing_time = combined_stats_extract['COMPILED_STATS']['XDAYWALLTIME']
        if weight == 1:
            target_time = existing_time - (total_time * (weight / 1000))
        else:
            target_time = existing_time - (total_xday_time * (weight / 1000))
        priority_dict[project] = round(target_time / 60 / 60, 2)
        log.debug(
            'Project is {} weight is {} existing time is {} so time delta is {}(s) or {}(h)'.format(
                project, weight, existing_time, target_time, round(target_time / 60 / 60, 4)
                )
            )
    if len(priority_dict) > 0:
        return sorted(priority_dict, key = priority_dict.get), priority_dict
    else:
        print('Error: Unable to find a highest priority project, ? Sleeping for 10 min')
        log.error('Unable to find a highest priority project, maybe all have been checked recently? Sleeping for 10 min')
        return [], {}


def get_project_mag_ratios_from_url(lookback_period: int = 30,
                                    project_resolver_dict: Dict[str, str] = None) -> Union[Dict[str, float], None]:
    """
    :param lookback_period: number of superblocks to look back to determine average
    :return: Dictionary w/ key as project URL and value as project mag ratio (mag per unit of RAC)
    """

    def project_name_to_url(searchname: str, project_resolver_dict: Dict[str, str]) -> Union[str, None]:
        all_projects = project_resolver_dict
        for found_project_name, project_url in project_resolver_dict.items():
            if found_project_name.upper() == searchname.upper():
                return project_url.upper()
        return None

    import requests as req
    import json
    projects = {}
    return_dict = {}
    mag_per_project = 0
    url = 'https://www.gridcoinstats.eu/API/simpleQuery.php?q=superblocks'
    try:
        resp = req.get(url)
    except Exception as e:
        print('Error retrieving project mag ratios from gridcoinstats.eu')
        return None
    loaded_json = json.loads(resp.text)
    for i in range(0, lookback_period):
        superblock = loaded_json[i]
        if i == 0:
            total_magnitude = superblock['total_magnitude']
            total_projects = superblock['total_projects']
            mag_per_project = total_magnitude / total_projects
        for project_name, project_stats in superblock['Contract Contents']['projects'].items():
            if project_name not in projects:
                if i == 0:
                    projects[project_name] = []
                else:
                    continue  # skip projects which are on greylist
            projects[project_name].append(project_stats['rac'])
    for project_name, project_racs in projects.items():
        average_rac = sum(project_racs) / len(project_racs)
        project_url = project_name_to_url(project_name, project_resolver_dict)
        return_dict[project_url] = mag_per_project / average_rac
    return return_dict


def url_to_just_domain_and_path(url: str) -> str:
    cleaned_url = url.upper().replace('HTTPS://', '').replace('HTTP://', '').replace('WWW.', '')
    if cleaned_url.endswith('/'):
        cleaned_project_url = cleaned_url[:-1]
    return cleaned_url


def get_project_from_dict(project_url: str, combined_stats: dict, debug_notes: str = '') -> Union[dict, int, None]:
    """
    project_url: A project URL to search for
    combined_stats: Dict to search through w/ project urls as keys. Can be ANY dict
    debug_notes: Output this when outputting that can't find url in debug str. IGNOREME to skip printing debug info
    Given a dict in format {projectname1:someinformation,projectname2:someinformation}, canonicalize project name by removing http/s/www
    and trailing slashes, ignoring capitalization, and returning someinformation, otherwise return none
    """
    cleaned_project_url = url_to_just_domain_and_path(project_url)
    if cleaned_project_url.endswith('/'):
        cleaned_project_url = cleaned_project_url[:-1]
    for found_project in combined_stats:
        if cleaned_project_url == found_project.upper() or cleaned_project_url in found_project.upper():
            return combined_stats[found_project]
    return None


def profitability_check(
    grc_price: float, exchange_fee: float, host_power_usage: float, grc_sell_price: Union[None, float], local_kwh: float,
    project: str, min_profit_per_hour: float, combined_stats: dict
    ) -> bool:
    """
    Returns True if crunching is profitable right now. False otherwise.
    """
    if not grc_sell_price:
        grc_sell_price = 0.00
    combined_stats_extract = get_project_from_dict(project, combined_stats, 'searching combined_stats in profitability_check')
    if not combined_stats_extract:
        log.error('Error: Unable to calculate profitability for project {} bc we have no stats for it'.format(project))
        return False
    revenue_per_hour = combined_stats_extract['COMPILED_STATS']['AVGMAGPERHOUR'] / 4 * max(grc_price, grc_sell_price)
    exchange_expenses = revenue_per_hour * exchange_fee
    expenses_per_hour = exchange_expenses + HOST_COST_PER_HOUR
    profit = revenue_per_hour - expenses_per_hour
    if profit > min_profit_per_hour:
        log.debug(
            'Determined project {} is profitable. Rev is {} expenses is {} profit is {}'.format(
                project, revenue_per_hour, expenses_per_hour, profit
                )
            )
        return True
    log.debug(
        'Determined project {} is NOT profitable. Rev is {} expenses is {} profit is {}'.format(
            project, revenue_per_hour, expenses_per_hour, profit
            )
        )
    return False


def in_preferred_projects(projecturl: str, preferred_projects: Dict[str, float]):
    cleaned_search_url = projecturl.upper().replace('HTTP://', '').replace('HTTPS://', '')
    for found_project in preferred_projects:
        cleaned_found = found_project.upper().replace('HTTP://', '').replace('HTTPS://', '')
        if cleaned_search_url in cleaned_found or cleaned_search_url == cleaned_found:
            return True
    return False


def benchmark_check(
    project_url: str, combined_stats: dict, benchmarking_minimum_wus: float, benchmarking_minimum_time: float,
    benchmarking_delay_in_days: float, skip_benchmarking: bool
    ) -> bool:
    """
    Returns True if we should force crunch this project for benchmarking reasons. False otherwise
    """

    def date_to_date(date: str) -> datetime.datetime:
        """
        Convert date from str to datetime
        """
        split = date.split('-')
        return datetime.datetime(int(split[2]), int(split[0]), int(split[1]))

    if skip_benchmarking:
        return False
    combined_stats_extract = get_project_from_dict(project_url, combined_stats, 'searching combined_stats in benchmark_check')
    if not combined_stats_extract:
        log.error('Unable to find project in benchmark_check'.format(project_url))
        return True
    if combined_stats_extract.get('COMPILED_STATS', {}).get('TOTALWALLTIME', 0) < benchmarking_minimum_time:
        log.debug('Forcing WU fetch on {} due to benchmarking_minimum_time'.format(project_url))
        return True
    if combined_stats_extract['COMPILED_STATS']['TOTALTASKS'] < benchmarking_minimum_wus:
        log.debug('Forcing WU fetch on {} due to benchmarking_minimum_tasks'.format(project_url))
        return True
    latest_date = datetime.datetime(1993, 1, 1)
    for date in combined_stats_extract['WU_HISTORY']:
        datetimed = date_to_date(date)
        if datetimed > latest_date:
            latest_date = datetimed
        delta = datetime.datetime.now() - latest_date
        if abs(delta.days) > benchmarking_delay_in_days:
            log.debug('Forcing WU fetch on {} due to benchmarking_delay_in_days'.format(project_url))
            return True
    return False


def save_stats(database: dict):
    with open('stats.json', 'w') as fp:
        json.dump(database, fp, default = json_default)


def custom_sleep(sleep_time: float, boinc_rpc_client):
    """
    A function to sleep and update the FTMTOTAL
    sleep_time: duration in minutes to sleep
    """
    log.debug('Sleeping for {}...'.format(sleep_time))
    elapsed = 0
    while elapsed < sleep_time:
        sleep(60)
        if loop.run_until_complete(is_boinc_crunching(boinc_rpc_client)):
            DATABASE['FTMTOTAL'] += 1
        # save database every ten minutes or at end of routine
        if str(elapsed).endswith('0') or elapsed + 1 >= sleep_time:
            save_stats(DATABASE)
        elapsed += 1


def json_default(obj):
    """
    For serializing datetimes to json
    """
    if isinstance(obj, datetime.datetime):
        return {'_isoformat': obj.isoformat()}
    raise TypeError('...')


def get_avg_mag_hr(combined_stats: dict) -> float:
    """
    Get average mag/hr over all projects to date
    """
    found_mag = []
    found_time = []
    for project_url, stats in combined_stats.items():
        total_hours = stats['COMPILED_STATS']['TOTALWALLTIME']
        total_mag = stats['COMPILED_STATS']['TOTALWALLTIME'] * stats['COMPILED_STATS']['AVGMAGPERHOUR']
        found_mag.append(total_mag)
        found_time.append(total_hours)
    found_sum = sum(found_time)
    found_mag = sum(found_mag)
    if found_sum == 0 or found_mag == 0:
        return 0
    average = found_mag / found_sum
    return average


def object_hook(obj):
    """
    For de-serializing datetimes from json
    """
    _isoformat = obj.get('_isoformat')
    if _isoformat is not None:
        return datetime.datetime.fromisoformat(_isoformat)
    return obj


def project_in_list_check(url: str, project_list: List[str]):
    """
    Case-insensitive way to check if URL is in list of URLs
    """
    cleaned = url.upper().replace('HTTPS://', '').replace('HTTP://', '')
    for project in project_list:
        cleaned_project = project.upper().replace('HTTPS://', '').replace('HTTP://', '')
        if cleaned == project:
            return True
        if cleaned in project:
            return True
    return False


def project_list_to_project_list(project_list: List[dict]) -> List[str]:
    """
    Convert get_project_list into a list of project URLs so we can perform 'in' comparisons
    """
    return_list = []
    for project in project_list:
        return_list.append(project['master_url'])
    return return_list


def boinc_loop(rpc_client = None, client_rpc_client = None, time: int = 0):
    """
    Main routine which manages BOINC
    :param rpc_client BOINC rpc client.
    :param client_rpc_client client BOINC rpc client
    :param time How long to crunch for.
    """
    if not client_rpc_client:
        client_rpc_client = rpc_client
    # these variables are referenced outside the loop (or in recursive calls of the loop) so should be made global
    global combined_stats
    global final_project_weights
    global total_preferred_weight
    global total_mining_weight
    global highest_priority_projects
    global priority_results
    mode = 'CLIENT'
    if mode not in DATABASE:
        DATABASE[mode] = {}

    def update_table(
        sleep_reason: str = DATABASE.get('TABLE_SLEEP_REASON', ''), status: str = DATABASE.get('TABLE_STATUS', '')
        ):
        """
        Function to update table printed to user.
        :param status = Most recent status "waiting for xfers, starting crunching on x, etc"
        """
        if SKIP_TABLE_UPDATES:
            return
        rename_dict = {
            'TOTALTASKS': 'TASKS',
            'TOTALTIME(HRS)': 'TIME',
            'TOTALCPUTIME(HRS)': 'CPUTIME',
            'AVGCREDITPERHOUR': 'CREDIT/HR',
            'AVGMAGPERHOUR': 'MAG/HR',
            'XDAYWALLTIME': 'R-WTIME',
            'AVGWALLTIME': 'ATIME',
            'AVGCREDITPERTASK': 'ACPT',
            'TOTALWALLTIME': 'WTIME',
            'TOTALCPUTIME': 'CPUTIME',
            'AVGCPUTIME': 'ACTIME'
            }
        ignore_list = ['MAGPERCREDIT']
        # generate table to print pretty
        os.system('cls' if os.name == 'nt' else 'clear')  # clear terminal
        table_dict = {}
        for project_url, stats_dict in combined_stats.items():
            table_dict[project_url] = {}
            priority_results_extract = get_project_from_dict(
                project_url, priority_results, 'searching priority_results in update_table'
                )
            if priority_results_extract:
                table_dict[project_url]['HOURSOFF'] = str(round(float(priority_results_extract), 3))
            else:
                table_dict[project_url]['HOURSOFF'] = str(round(float(0), 3))
            for stat_name, stat_value in stats_dict['COMPILED_STATS'].items():
                if stat_name in ignore_list:
                    continue
                renamed = stat_name
                if stat_name in rename_dict:
                    renamed = rename_dict[stat_name]
                rounding = 2
                if stat_name == 'MAGPERCREDIT':
                    rounding = 5
                if stat_name == 'AVGMAGPERHOUR':
                    rounding = 3
                table_dict[project_url][renamed] = str(round(float(stat_value), rounding))
            final_project_weights_extract = get_project_from_dict(
                project_url, final_project_weights, 'searching final_project_weights in update_table'
                )
            if final_project_weights_extract:
                table_dict[project_url]['WEIGHT'] = str(final_project_weights_extract)
            else:
                table_dict[project_url]['WEIGHT'] = 'NA'
        print_table(table_dict, sortby = 'GRC/HR', sleep_reason = sleep_reason, status = status)

    while True:
        # Re-authorize in case we have become de-authorized since last run. This is put in a try loop b/c sometimes it throws exceptions
        while True:
            try:
                authorize_response = loop.run_until_complete(rpc_client.authorize())
                BOINC_PROJECT_LIST, BOINC_PROJECT_NAMES = loop.run_until_complete(
                    get_attached_projects(rpc_client)
                    )  # we need to re-fetch this as it's different for dev and client
            except Exception as e:
                print_and_log('Transient error connecting to BOINC, sleeping 30s', 'ERROR')
                sleep(30)
            else:
                break

        # If we haven't re-calculated stats recently enough, do it
        stats_calc_delta = datetime.datetime.now() - DATABASE.get('STATSLASTCALCULATED', datetime.datetime(1997, 3, 3))
        if ((abs(stats_calc_delta.days) * 24 * 60) +
            (abs(stats_calc_delta.seconds) / 60)) > recalculate_stats_interval:  #only re-calculate stats every x minutes
            log.debug('Calculating stats..')
            DATABASE['STATSLASTCALCULATED'] = datetime.datetime.now()
            combined_stats = config_files_to_stats(boinc_data_dir)
            # total_time = combined_stats_to_total_time(combined_stats) # Not sure what this line did but commented out, we'll see if anything breaks
            combined_stats, final_project_weights, total_preferred_weight, total_mining_weight = generate_stats(
                APPROVED_PROJECT_URLS = APPROVED_PROJECT_URLS,
                preferred_projects = preferred_projects,
                ignored_projects = ignored_projects,
                quiet = True,
                ignore_unattached = True,
                attached_list = BOINC_PROJECT_LIST,
                mag_ratios = mag_ratios
                )
            # Get list of projects ordered by priority
            highest_priority_projects, priority_results = get_highest_priority_project(
                combined_stats = combined_stats,
                project_weights = final_project_weights,
                attached_projects = BOINC_PROJECT_LIST,
                quiet = True
                )
            log.debug('Highest priority projects are: ' + str(highest_priority_projects))
            # print some pretty stats
            update_table()

        log.info("Highest priority project is {}".format(highest_priority_projects[0]))
        loop.run_until_complete(nnt_all_projects(rpc_client))  # NNT all projects

        # If we haven't checked GRC prices in a while, do it
        price_check_delta = datetime.datetime.now() - DATABASE.get('GRCPRICELASTCHECKED', datetime.datetime(1993, 3, 3))
        price_check_calc = ((abs(price_check_delta.days) * 24 * 60) + (abs(price_check_delta.seconds) / 60))
        if price_check_calc > max(price_check_interval, 60):
            grc_price = get_grc_price()
            DATABASE['GRCPRICELASTCHECKED'] = datetime.datetime.now()
            DATABASE['GRCPRICE'] = grc_price
        else:
            grc_price = DATABASE['GRCPRICE']
        # Check profitability of all projects, if none profitable (and user doesn't want unprofitable crunching), sleep for 1hr
        if only_BOINC_if_profitable:
            profitability_list = []
            for project in highest_priority_projects:
                profitability_result = profitability_check(
                    grc_price = grc_price,
                    exchange_fee = exchange_fee,
                    host_power_usage = host_power_usage,
                    grc_sell_price = grc_sell_price,
                    local_kwh = local_kwh,
                    project = project,
                    min_profit_per_hour = min_profit_per_hour,
                    combined_stats = combined_stats
                    )
                benchmarking_result = benchmark_check(
                    project_url = project,
                    combined_stats = combined_stats,
                    benchmarking_minimum_wus = benchmarking_minimum_wus,
                    benchmarking_minimum_time = benchmarking_minimum_time,
                    benchmarking_delay_in_days = benchmarking_delay_in_days,
                    skip_benchmarking = skip_benchmarking
                    )
                profitability_list.append(profitability_result)
                profitability_list.append(benchmarking_result)
            if True not in profitability_list:
                log.info(
                    'No projects currently profitable and no benchmarking required, sleeping for 1 hour and killing all non-started tasks'
                    )
                tasks_list = get_task_list(rpc_client)
                kill_all_unstarted_tasks(rpc_client = rpc_client, task_list = tasks_list)
                nnt_all_projects(rpc_client)
                DATABASE['TABLE_SLEEP_REASON'
                        ] = 'No profitable projects and no benchmarking required, sleeping 1 hr, killing all non-started tasks'
                update_table()
                sleep(60 * 60)
                continue

        # If we have enabled temperature control, verify that crunching is allowed at current temp
        if enable_temp_control:
            # Get BOINC's starting CPU and GPU modes
            existing_mode_info = loop.run_until_complete(run_rpc_command(rpc_client, 'get_cc_status'))
            existing_cpu_mode = existing_mode_info['task_mode']
            existing_gpu_mode = str(existing_mode_info['gpu_mode'])
            if existing_cpu_mode in CPU_MODE_DICT:
                existing_cpu_mode = CPU_MODE_DICT[existing_cpu_mode]
            else:
                print_and_log('Error: Unknown cpu mode {}'.format(existing_cpu_mode), 'ERROR')
            if existing_gpu_mode in GPU_MODE_DICT:
                existing_gpu_mode = GPU_MODE_DICT[existing_gpu_mode]
            else:
                print_and_log('Error: Unknown gpu mode {}'.format(existing_gpu_mode), "ERROR")

            # If temp is too high:
            if not temp_check():
                while True:  # Keep sleeping until we pass a temp check
                    log.debug('Sleeping due to temperature')
                    # Put BOINC into sleep mode, automatically reverting if script closes unexpectedly
                    sleep_interval = str(int(((60 * temp_sleep_time) + 60)))
                    loop.run_until_complete(run_rpc_command(rpc_client, 'set_run_mode', 'never', sleep_interval))
                    loop.run_until_complete(run_rpc_command(rpc_client, 'set_gpu_mode', 'never', sleep_interval))
                    DATABASE['TABLE_SLEEP_REASON'] = 'Temperature'
                    update_table()
                    sleep(60 * temp_sleep_time)
                    if temp_check():
                        # Reset to initial crunching modes now that temp is satisfied
                        loop.run_until_complete(run_rpc_command(rpc_client, 'set_run_mode', existing_cpu_mode))
                        loop.run_until_complete(run_rpc_command(rpc_client, 'set_gpu_mode', existing_gpu_mode))
                        break

        # loop through each project in order of priority and request new tasks if not backed off
        # stopping looping if cache becomes full
        dont_nnt = None
        project_loop = highest_priority_projects
        for highest_priority_project in project_loop:
            boincified_url = resolve_boinc_url_new(highest_priority_project)
            benchmark_result = benchmark_check(
                project_url = highest_priority_project,
                combined_stats = combined_stats,
                benchmarking_minimum_wus = benchmarking_minimum_wus,
                benchmarking_minimum_time = benchmarking_minimum_time,
                benchmarking_delay_in_days = benchmarking_delay_in_days,
                skip_benchmarking = skip_benchmarking
                )
            profitability_result = profitability_check(
                grc_price = grc_price,
                exchange_fee = exchange_fee,
                host_power_usage = host_power_usage,
                grc_sell_price = grc_sell_price,
                local_kwh = local_kwh,
                project = highest_priority_project,
                min_profit_per_hour = min_profit_per_hour,
                combined_stats = combined_stats
                )
            if only_BOINC_if_profitable and not benchmark_result and not profitability_result:
                DATABASE['TABLE_STATUS'] = 'No fetch for {} bc not profitable'.format(highest_priority_project)
                update_table()
                log.info(
                    'Skipping work fetch for {} bc not profitable and only_boinc_if_profitable is set to true'
                    .format(highest_priority_project)
                    )
                continue
            # If user has set to only mine highest mag project if profitable and it's not profitable or in benchmarking mode, skip
            if only_mine_if_profitable and not profitability_result and final_project_weights[highest_priority_project] != 1:
                DATABASE['TABLE_STATUS'
                        ] = 'Skipping work fetch for {} bc not profitable and only_mine_if_profitable set to true'.format(
                            highest_priority_project
                            )
                update_table()
                log.info(
                    'Skipping work fetch for {} bc not profitable and only_mine_if_profitable set to true'
                    .format(highest_priority_project)
                    )
                continue

            highest_priority_project = resolve_boinc_url(
                highest_priority_project, ALL_BOINC_PROJECTS
                )  # make sure we are using correct URL, BOINC requires capitalization to be exact
            if highest_priority_project.upper() not in DATABASE[mode]:
                DATABASE[mode][highest_priority_project.upper()] = {}
            # skip checking project if we have a backoff counter going and it hasn't been long enough
            time_since_last_project_check = datetime.datetime.now() - DATABASE[mode][
                highest_priority_project.upper()].get('LAST_CHECKED', datetime.datetime(1997, 6, 21, 18, 25, 30))
            minutes_since_last_project_check = time_since_last_project_check.seconds / 60
            if minutes_since_last_project_check < DATABASE[mode].get(highest_priority_project.upper(), {}).get('BACKOFF', 0):
                DATABASE['TABLE_STATUS'] = 'Skipping {} due to backoff period...'.format({highest_priority_project})
                update_table()
                log.debug(
                    'Skipping project {} due to backoff period... minutes_since is {}'.format(
                        highest_priority_project, minutes_since_last_project_check
                        )
                    )
                continue
            DATABASE['TABLE_STATUS'] = 'Waiting for xfers to complete..'
            update_table()
            log.info('Waiting for any xfers to complete...')
            dl_response = wait_till_no_xfers(rpc_client)  # wait until all network activity has concluded
            project_name = ALL_BOINC_PROJECTS[highest_priority_project]
            DATABASE['TABLE_STATUS'] = 'Allowing new tasks & updating {}'.format(project_name)
            log.info('Allowing new tasks and updating {}'.format(highest_priority_project))
            update_table()
            allow_response = loop.run_until_complete(
                run_rpc_command(rpc_client, 'project_allowmorework', 'project_url', boincified_url)
                )
            update_response = loop.run_until_complete(
                run_rpc_command(rpc_client, 'project_update', 'project_url', boincified_url)
                )  # update project
            log.debug('Requesting work from {} added to debug no new tasks bug' + str(boincified_url))
            log.debug('Update response is {}'.format(update_response))
            sleep(15)  # give BOINC time to update w project, I don't know a less hacky way to do this, suggestions are welcome
            DATABASE[mode][highest_priority_project.upper()]['LAST_CHECKED'] = datetime.datetime.now()
            # check if project should be backed off. If so, back it off.
            # This is an exponentially increasing backoff with a maximum time of 1 day
            # Projects are backed off if they request it, if they are unresponsive/down, or if no work is available
            backoff_response = loop.run_until_complete(check_log_entries_for_backoff(rpc_client, project_name = project_name))
            if backoff_response:
                if DATABASE[mode][highest_priority_project.upper()].get('BACKOFF'):
                    DATABASE[mode][highest_priority_project.upper()
                                  ]['BACKOFF'] = min(DATABASE[mode][highest_priority_project.upper()]['BACKOFF'] * 2, 1440)
                else:
                    DATABASE[mode][highest_priority_project.upper()]['BACKOFF'] = min_recheck_time
            else:
                DATABASE[mode][highest_priority_project.upper()]['BACKOFF'] = 0
                log.debug('Waiting for any xfers to complete...')
                dl_response = wait_till_no_xfers(rpc_client)  # wait until all network activity has concluded

                if not dont_nnt:  # if we didn't get a backoff signal and we haven't picked a project to leave non-NNTed during sleeping of loop, pick this one for that purpose
                    dont_nnt = highest_priority_project.upper()

            # re-NNT all projects
            nnt_response = loop.run_until_complete(nnt_all_projects(rpc_client))  # NNT all projects

            # Check logs to see if both work caches are full
            cache_full = loop.run_until_complete(check_log_entries(rpc_client, project_name = project_name))
            log.debug('checking log response for work cache status....')

            # If BOINC job cache is full, stop asking projects for work
            if cache_full:
                DATABASE['TABLE_SLEEP_REASON'] = 'BOINC work cache full...'
                update_table()
                break

        # Allow highest non-backedoff project to be non-NNTd.
        # This enables BOINC to fetch work if it's needed before our sleep period elapses
        if dont_nnt:
            allow_this_project = resolve_boinc_url_new(dont_nnt)
            allow_response = loop.run_until_complete(
                run_rpc_command(rpc_client, 'project_allowmorework', 'project_url', allow_this_project)
                )
        custom_sleep(30, rpc_client)  # There's no reason to loop through all projects more than once every 30 minutes


def print_and_log(msg: str, log_level: str) -> None:
    """
    Print a message and add it to the log at log_level. Valid log_levels are DEBUG, INFO, WARNING, ERROR
    """
    print(msg)
    if log_level == 'DEBUG':
        log.debug(msg)
    elif log_level == 'WARNING':
        log.warning(msg)
    elif log_level == 'INFO':
        log.info(msg)
    elif log_level == 'ERROR':
        log.error(msg)
    else:
        print('Being asked to log at an unknown level: {}'.format(log_level))


def create_default_database() -> Dict[str, Any]:
    DATABASE: Dict[str, Any] = {}
    DATABASE['FTMTOTAL'] = 0
    DATABASE['TABLE_STATUS'] = ''
    DATABASE['TABLE_SLEEP_REASON'] = ''
    return DATABASE


if __name__ == '__main__':
    wallet_running = True  # switches to false if we have issues connecting

    # Verify we are in appropriate python environment
    python_major = sys.version_info.major
    python_minor = sys.version_info.minor
    if python_major < 3:
        print(
            'Error: This program requires python 3.6 or higher to run, you are running it as Python {}'.format(
                platform.python_version()
                )
            )
        input('Press enter to exit')
        quit()
    elif python_major == 3 and python_minor < 6:
        print(
            'Error: This program requires python 3.6 or higher to run, you are running it as Python {}'.format(
                platform.python_version()
                )
            )
        input('Some things may not work as expected. Press enter to continue')
    del python_minor
    del python_major
    log.debug('Python version {}'.format(platform.python_version()))

    print('Welcome to GridcoinHelper.')
    print('This tool is a modified version of FindTheMag2 written by Macesuted.')

    # Load long-term stats
    if os.path.exists('stats.json'):
        try:
            with open('stats.json') as json_file:
                DATABASE: Dict[str, Any] = json.load(json_file, object_hook = object_hook)
        except Exception as e:
            if os.path.exists('stats.json.backup'):
                print('Error opening stats file, trying backup...')
                log.error('Error opening stats file, trying backup...')
                try:
                    with open('stats.json.backup') as json_file:
                        DATABASE: Dict[str, Any] = json.load(json_file, object_hook = object_hook)
                except:
                    print_and_log('Error opening stats file, making new one...', 'ERROR')
                    DATABASE = create_default_database()
                    save_stats(DATABASE)
            else:
                print_and_log('Error loading stats file. Making new one...', 'ERROR')
                shutil.copy('stats.json', 'stats.json.corrupted')
                DATABASE = create_default_database()
                save_stats(DATABASE)
    else:
        DATABASE = create_default_database()
        save_stats(DATABASE)

    # These vars should reset and/or checked each run
    DATABASE['TABLE_STATUS'] = ''
    DATABASE['TABLE_SLEEP_REASON'] = ''
    if 'FTMTOTAL' not in DATABASE:
        DATABASE['FTMTOTAL'] = 0

    signal.signal(signal.SIGINT, safe_exit)  # Capture ctrl+c from client to exit gracefully
    update_check()  # Check for updates to FTM
    combined_stats = {}
    APPROVED_PROJECT_URLS = []
    # combined_stats has format:
    #    COMBINED_STATS_EXAMPLE = {
    #        'HTTP://PROJECT.COM/PROJECT': {
    #            'COMPILED_STATS': {
    #                'AVGWALLTIME': 30.01, 'AVGCPUTIME': 10.02, 'TOTALTASKS': 51, 'TOTALWALLTIME': 223311.34,
    #                'AVGCREDITPERHOUR': 31.2, 'XDAYWALLTIME': 30, 'AVGCREDITPERTASK': 32.12, 'AVGMAGPERHOUR': 32.1, 'TOTALCPUTIME':300010.10},
    #            'CREDIT_HISTORY': {
    #                '11-29-21': {'CREDITAWARDED':100.54},
    #                '11-28-21': {'CREDITAWARDED':100.21},
    #            },
    #            'WU_HISTORY': {
    #                '07-31-2021':{'STARTTIME': '1627765997', 'ESTTIME': '6128.136145', 'CPUTIME': '3621.724000',
    #                 'ESTIMATEDFLOPS': '30000000000000', 'TASKNAME': 'wu_sf3_DS-16x271-9_Grp218448of1000000_0',
    #                 'WALLTIME': '3643.133927', 'EXITCODE': '0'},
    #                '07-29-2021': {'STARTTIME': '1627765996', 'ESTTIME': '6128.136145', 'CPUTIME': '3621.724000',
    #                               'ESTIMATEDFLOPS': '30000000000000',
    #                               'TASKNAME': 'wu_sf3_DS-16x271-9_Grp218448of1000000_0',
    #                               'WALLTIME': '3643.133927', 'EXITCODE': '0'},
    #            }
    #        },
    #    }

    # Define starting parameters
    found_platform = platform.system()
    if not boinc_data_dir:
        if found_platform == 'Linux':
            if os.path.isdir('/var/lib/boinc-client'):
                boinc_data_dir = '/var/lib/boinc-client'
            else:
                boinc_data_dir = os.path.join(Path.home(), 'BOINC/')
        elif found_platform == 'Darwin':
            boinc_data_dir = os.path.join('/Library/Application Support/BOINC Data/')
        else:
            boinc_data_dir = 'C:\ProgramData\BOINC\\'
    if not gridcoin_data_dir:
        if found_platform == 'Linux':
            gridcoin_data_dir = os.path.join(Path.home(), '.GridcoinResearch/')
        elif found_platform == 'Darwin':
            gridcoin_data_dir = os.path.join(Path.home(), 'Library/Application Support/GridcoinResearch/')
        else:
            gridcoin_data_dir = os.path.join(Path.home(), 'AppData\Roaming\GridcoinResearch\\')

    # check that directories exist
    log.info('Guessing BOINC data dir is ' + str(boinc_data_dir))
    if not os.path.isdir(boinc_data_dir):
        print(
            'BOINC data dir does not appear to exist. If you have it in a non-standard location, please edit config.py so we know where to look'
            )
        log.error(
            'BOINC data dir does not appear to exist. If you have it in a non-standard location, please edit config.py so we know where to look'
            )
        input('Press enter to exit')
        quit()
    log.info('Guessing Gridcoin data dir is ' + str(gridcoin_data_dir))
    if not os.path.isdir(gridcoin_data_dir):
        print(
            'Gridcoin data dir does not appear to exist. If you have it in a non-standard location, please edit config.py so we know where to look'
            )
        log.error(
            'Gridcoin data dir does not appear to exist. If you have it in a non-standard location, please edit config.py so we know where to look'
            )
        input('Press enter to continue or CTRL+C to quit')
        wallet_running = False
    override_path = os.path.join(boinc_data_dir, 'global_prefs_override.xml')
    override_dest_path = os.path.join(os.getcwd(), 'global_prefs_override_backup.xml')

    try:
        os.access(override_path, os.W_OK)
    except Exception as e:
        print_and_log(
            'This program does not have write access to your BOINC config file, meaning it can\'t reset settings back to your original ones upon close',
            'ERROR'
            )
        print_and_log("Linux users try 'sudo chown your_username {}' to fix this error".format(override_path), 'INFO')
        if not SCRIPTED_RUN:
            input('Press enter to continue')

    # auto-detect password for BOINC RPC if it exists and user didn't know
    # BOINC on Windows automatically generates an RPC password
    auth_location = os.path.join(boinc_data_dir, 'gui_rpc_auth.cfg')
    if not boinc_password:
        try:
            if os.path.exists(auth_location):
                with open(auth_location, 'r') as file:
                    data = file.read().rstrip()
                    if data != '':
                        boinc_password = data
        except Exception as e:
            # This error can generally be disregarded on Linux/OSX
            if 'WINDOWS' in found_platform.upper():
                print('Error reading boinc RPC file at {}: {}'.format(auth_location, e))
                log.error('Error reading boinc RPC file at {}: {}'.format(auth_location, e))
            else:
                log.debug('Error reading boinc RPC file at {}: {}'.format(auth_location, e))

    # Check that project weights make sense
    total_found_values = 0
    for url, found_value in preferred_projects.items():
        total_found_values += found_value
    if total_found_values != 100 and len(preferred_projects) > 0:
        print('Warning: The weights of your preferred projects do not add up to 100! Quitting.')
        log.error('Warning: The weights of your preferred projects do not add up to 100! Quitting.')
        input('Press enter to exit')
        quit()

    # Establish connections to BOINC and Gridcoin clients, get basic info
    boinc_client = None
    grc_client = None
    gridcoin_conf = None
    loop = asyncio.get_event_loop()
    try:
        boinc_client = BoincClientConnection(config_dir = boinc_data_dir)
    except Exception as e:
        print('Unable to connect to a BOINC client. Are you sure BOINC is running? Error ' + str(e))
        log.error('Unable to connect to a BOINC client. Are you sure BOINC is running? Error ' + str(e))
        input('Press enter to exit')
        quit()
    if wallet_running:
        try:
            gridcoin_conf = get_config_parameters(gridcoin_data_dir)
        except Exception as e:
            print('Error parsing gridcoin config file in directory: ' + gridcoin_data_dir + ' Error: ' + str(e))
            log.error('Error parsing gridcoin config file in directory: ' + gridcoin_data_dir + ' Error: ' + str(e))
            wallet_running = False
            rpc_user = None
            gridcoin_rpc_password = None
            rpc_port = None
        else:
            #Get project lists from Gridcoin wallet
            rpc_user = gridcoin_conf.get('rpcuser')
            gridcoin_rpc_password = gridcoin_conf.get('rpcpassword')
            rpc_port = gridcoin_conf.get('rpcport')
        if not rpc_user or not gridcoin_rpc_password or not rpc_port:
            print(
                'Error: Gridcoin wallet is not configured to accept RPC commands based on config file from '
                + str(gridcoin_data_dir)
                )
            log.error(
                'Error: Gridcoin wallet is not configured to accept RPC commands based on config file from '
                + str(gridcoin_data_dir)
                )
            print('RPC commands enable us to talk to the Gridcoin client and get information about project magnitude ratios')
            print('Would you like us to automatically configure your Gridcoin client to accept RPC commands?')
            print('It will be configured to only accept commands from your machine.')
            print(
                'If you do not enable this, this script can only update its information about project magnitudes once a day through an external website'
                )
            print('This can cause inefficient crunching and is not advised')
            print('Please answer "Y" or "N" without quotes. Then press the enter key')
            answer = input("")
            log.debug('User input: ' + answer)
            while answer not in ['Y', 'N']:
                print('Error: Y or N not entered. Try again please :)')
                answer = input("")
            if answer == "N":
                print('Ok, we won\'t')
            else:
                with open(os.path.join(gridcoin_data_dir, 'gridcoinresearch.conf'), "a") as myfile:
                    from random import choice
                    from string import ascii_uppercase
                    from string import ascii_lowercase
                    from string import digits
                    rpc_user = ''.join(choice(ascii_uppercase) for i in range(8))
                    gridcoin_rpc_password = ''.join(choice(ascii_uppercase + ascii_lowercase + digits) for i in range(12))
                    rpc_port = 9876
                    print('Your RPC username is: ' + rpc_user)
                    print('Your RPC password is: ' + gridcoin_rpc_password)
                    print('You don\'t need to remember these.')
                    print('Modifying config file...')
                    myfile.write("rpcport=9876\n")
                    myfile.write("server=1\n")
                    myfile.write("rpcuser=" + rpc_user + '\n')
                    myfile.write("rpcpassword=" + gridcoin_rpc_password + '\n')
                print('Alright, we\'ve modified the config file. Please restart the gridcoin wallet.')
                print('Once it\'s loaded and --fully-- synced, press enter to continue')
                input('')

    #Get project list from BOINC
    rpc_client = None
    try:
        rpc_client = loop.run_until_complete(
            setup_connection(boinc_ip, boinc_password, boinc_port)
            )  # setup BOINC RPC connection
    except Exception as e:
        print_and_log('Error: Unable to connect to BOINC client, quitting now', 'ERROR')
        quit()
    if not rpc_client:  # this was just added so pycharm would stop complaining about rpc_client not being declared
        print_and_log('Error connecting to BOINC client, quitting now', 'ERROR')
        quit()
    BOINC_PROJECT_LIST, BOINC_PROJECT_NAMES = loop.run_until_complete(
        get_attached_projects(rpc_client)
        )  # get project list from BOINC client directly. This is needed for correct capitalization
    ALL_BOINC_PROJECTS = loop.run_until_complete(get_all_projects(rpc_client))

    # Get project list from Gridcoin wallet and/or gridcoinstats
    check_sidestake_results = False
    foundation_address = 'bc3NA8e8E3EoTL1qhRmeprbjWcmuoZ26A2'
    developer_address = 'RzUgcntbFm8PeSJpauk6a44qbtu92dpw3K'
    try:
        grc_client = GridcoinClientConnection(rpc_user = rpc_user, rpc_port = rpc_port, rpc_password = gridcoin_rpc_password)
        APPROVED_PROJECT_URLS = grc_client.get_approved_project_urls()
        mag_ratios = get_project_mag_ratios(grc_client, lookback_period)
    except Exception as e:
        print_and_log('Unable to connect to Gridcoin wallet. Assuming it doesn\'t exist. Error: ', 'ERROR')
        log.error('{}'.format(e))
        print('It is suggested to install the Gridcoin wallet for the most up-to-date magnitude information')
        print('Otherwise, we will fetch data from gridcoinstats.eu which is limited to once per day')
        log.warning('Unable to connect to gridcoin wallet! {} Trying web-based option...'.format(e))
        wallet_running = False
        try:
            APPROVED_PROJECT_URLS, project_resolver_dict = get_approved_project_urls_web()
            mag_ratios = get_project_mag_ratios_from_url(project_resolver_dict = project_resolver_dict)
        except Exception as e:
            print_and_log('Error getting project URL list from URL. Are you sure it\'s open? Error: ' + str(e), 'ERROR')
            input('Press enter to exit')
            quit()
    else:
        if not SCRIPTED_RUN:
            # Check sidestakes
            check_sidestake_results = check_sidestake(gridcoin_conf, foundation_address, 1)
            sidestake_check(check_sidestake_results, 'FOUNDATION', foundation_address)
            check_sidestake_results = check_sidestake(gridcoin_conf, developer_address, 1)
            sidestake_check(check_sidestake_results, 'DEVELOPER', developer_address)
            print(
                'Welcome to FindTheMag and thank you for trying out this tool. Your feedback and suggestions are welcome on the github page : )'
                )
            check_sidestake_results = check_sidestake(gridcoin_conf, developer_address, 1)

    # Get project list from BOINC
    try:
        ALL_PROJECT_URLS = boinc_client.get_project_list()
    except Exception as e:
        print_and_log('Error getting project URL list from BOINC ' + str(e), 'ERROR')

    combined_stats, final_project_weights, total_preferred_weight, total_mining_weight = generate_stats(
        APPROVED_PROJECT_URLS = APPROVED_PROJECT_URLS,
        preferred_projects = preferred_projects,
        ignored_projects = ignored_projects,
        quiet = True,
        mag_ratios = mag_ratios
        )
    log.debug('Printing pretty stats...')
    # calculate starting efficiency stats
    if 'STARTMAGHR' not in DATABASE:
        DATABASE['STARTMAGHR'] = get_avg_mag_hr(combined_stats)
    else:
        original_avg_mag_hr = DATABASE['STARTMAGHR']
        current_avg_mag_hr = get_avg_mag_hr(combined_stats)
        if current_avg_mag_hr > original_avg_mag_hr and original_avg_mag_hr != 0:
            percent_increase = ((current_avg_mag_hr - original_avg_mag_hr) / original_avg_mag_hr) * 100
            print(
                'When you started using this tool, your average mag/hr was: {:.4f} now it is {:.4f}, a {}% increase!'.format(
                    original_avg_mag_hr, current_avg_mag_hr, percent_increase
                    )
                )
        else:
            print(
                'When you started using this tool, your average mag/hr was: {:.4f} now it is {:.4f}'.format(
                    original_avg_mag_hr, current_avg_mag_hr
                    )
                )
    #generate table to print pretty
    table_dict = {}
    for project_url, stats_dict in combined_stats.items():
        table_dict[project_url] = {}
        for stat_name, stat_value in stats_dict['COMPILED_STATS'].items():
            rounding = 2
            if stat_name == 'MAGPERCREDIT':
                rounding = 5
            table_dict[project_url][stat_name] = str(round(float(stat_value), rounding))
    print('')
    if len(table_dict) > 0:
        print('SOME PRETTY STATS JUST FOR YOU, SORTED BY AVERAGE MAG/HOUR')
        print_table(table_dict, sortby = 'AVGMAGPERHOUR')
    else:
        print('Not enough stats to print a table of them yet, guessing this is a new BOINC install?')
    print('Total project weight will be 1000. We will reserve a minimum .01% of processing power for monitoring each project')
    print_and_log('Total weight for preferred projects is ' + str(round(float(total_preferred_weight), 2)), 'INFO')
    print_and_log('Total weight for mining projects is ' + str(round(float(total_mining_weight), 2)), 'INFO')
    print_and_log('FINAL SUGGESTED PROJECT WEIGHTS', 'INFO')
    for project, weight in final_project_weights.items():
        print_and_log(project.lower() + ': ' + str(weight), 'INFO')
    if check_sidestake_results:
        print('~~---***Wow THANK YOU for sidestaking to our development. You rock!***---~~~')
        print('Yeeeehaw! We\'re going to the pony store!')
        print(
            """
---             ,--,
----      _ ___/ /\|
-----    ;( )__, )
-----   ; //   '--;
----      \     |
---        v    v"""
            )
    else:
        print(
            'If you\'d like to say thank you to the developers of this tool, please help us buy our next round of energy drinks by sending GRC to:'
            )
        print(developer_address)
    if not control_boinc and not SCRIPTED_RUN:
        input('Press enter key or CTRL+C to quit')
        quit()
    else:
        if not SCRIPTED_RUN:
            print('Press enter key to start controlling BOINC. Press Ctrl+C to quit')
    if not SCRIPTED_RUN:
        answer = input("")
    print_and_log('Starting control of BOINC...', 'DEBUG')

    # Backup user preferences.
    try:
        shutil.copy(override_path, override_dest_path)
    except Exception as e:
        log.warning(
            'global_prefs_override.xml does not appear to exist, not backing up. Some users may not have one. Error: {}'
            .format(e)
            )

    verification_result = loop.run_until_complete(verify_boinc_connection(rpc_client))
    if not verification_result:
        print_and_log(
            'Error connecting to BOINC client, does your gui_rpc_auth.cfg specify a password or a non-standard port?\n If so, be sure to include it in your config.py',
            'ERROR'
            )
        print('You can find your gui_rpc_auth.cfg at {}'.format(auth_location))
        print('Linux users: make sure your username is in the BOINC group so FTM can access your BOINC config file')
        print('sudo usermod -aG boinc your_username_here')
        print('Note that you will need to restart your computer after changing your group permissions')
        answer = input('Press enter to quit')
        quit()
    loop.run_until_complete(prefs_check(rpc_client))
    # NNT all projects
    nnt_response = loop.run_until_complete(nnt_all_projects(rpc_client))
    # Abort unstarted tasks if the user requested it
    if abort_unstarted_tasks:
        tasks_list = loop.run_until_complete(get_task_list(rpc_client))
        loop.run_until_complete(kill_all_unstarted_tasks(rpc_client, task_list = tasks_list))
    priority_results = {}
    highest_priority_project = ''
    highest_priority_projects = []
    DATABASE['STATSLASTCALCULATED'] = datetime.datetime(
        1997, 3, 3
        )  # force calculation of stats at first run since they are not cached in DB
    # While we don't have enough tasks, continue cycling through project list and updating. If we have cycled through all projects, get_highest_priority_project will stall to prevent requesting too often
    boinc_loop(False, rpc_client)
    # Restore user prefs
    safe_exit(None, None)
